{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Source: https://www.kaggle.com/dragonoken/churn-modelling-with-pytorch/notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reviewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>course</th>\n",
       "      <th>completed</th>\n",
       "      <th>upgraded</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>job</th>\n",
       "      <th>major</th>\n",
       "      <th>...</th>\n",
       "      <th>completion_year</th>\n",
       "      <th>completion_month</th>\n",
       "      <th>completion_day</th>\n",
       "      <th>completion_hour</th>\n",
       "      <th>completion_minute</th>\n",
       "      <th>upgrade_year</th>\n",
       "      <th>upgrade_month</th>\n",
       "      <th>upgrade_day</th>\n",
       "      <th>upgrade_hour</th>\n",
       "      <th>upgrade_minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>284921427</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>284926400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>284946595</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>285755462</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>285831220</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  course  completed  upgraded  gender  country  age  education  \\\n",
       "0  284921427       1          0         0       0        0    0          0   \n",
       "1  284926400       1          0         0       0        0    0          0   \n",
       "2  284946595       1          0         0       0        0    0          0   \n",
       "3  285755462       1          0         0       0        0    0          0   \n",
       "4  285831220       1          1         0       0        0    0          0   \n",
       "\n",
       "   job  major  ...  completion_year  completion_month  completion_day  \\\n",
       "0    0      0  ...                0                 0               0   \n",
       "1    0      0  ...                0                 0               0   \n",
       "2    0      0  ...                0                 0               0   \n",
       "3    0      0  ...                0                 0               0   \n",
       "4    0      0  ...             2011                 5              29   \n",
       "\n",
       "   completion_hour  completion_minute  upgrade_year  upgrade_month  \\\n",
       "0                0                  0             0              0   \n",
       "1                0                  0             0              0   \n",
       "2                0                  0             0              0   \n",
       "3                0                  0             0              0   \n",
       "4               15                 58             0              0   \n",
       "\n",
       "   upgrade_day  upgrade_hour  upgrade_minute  \n",
       "0            0             0               0  \n",
       "1            0             0               0  \n",
       "2            0             0               0  \n",
       "3            0             0               0  \n",
       "4            0             0               0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\kusalh\\\\Documents\\\\Courses\")\n",
    "BD = pd.read_csv('Courses.csv')\n",
    "BD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving on to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>completed</th>\n",
       "      <th>upgraded</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>job</th>\n",
       "      <th>major</th>\n",
       "      <th>logged_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24995</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24996</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24997</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24998</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24999</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       course  completed  upgraded  gender  country  age  education  job  \\\n",
       "0           1          0         0       0        0    0          0    0   \n",
       "1           1          0         0       0        0    0          0    0   \n",
       "2           1          0         0       0        0    0          0    0   \n",
       "3           1          0         0       0        0    0          0    0   \n",
       "4           1          1         0       0        0    0          0    0   \n",
       "...       ...        ...       ...     ...      ...  ...        ...  ...   \n",
       "24995       3          0         0       0        0    0          0    0   \n",
       "24996       3          0         0       0        0    0          0    0   \n",
       "24997       3          0         0       0        0    0          0    0   \n",
       "24998       3          0         0       0        0    0          0    0   \n",
       "24999       3          0         0       0        0    0          0    0   \n",
       "\n",
       "       major  logged_location  \n",
       "0          0                0  \n",
       "1          0                0  \n",
       "2          0                0  \n",
       "3          0               56  \n",
       "4          0                0  \n",
       "...      ...              ...  \n",
       "24995      0               56  \n",
       "24996      0               56  \n",
       "24997      0              171  \n",
       "24998      0               56  \n",
       "24999      0               51  \n",
       "\n",
       "[25000 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date values are not important unless engineered\n",
    "DF = BD.drop([\n",
    "                'ID',\n",
    "#                 'upgraded',\n",
    "#             ], axis=1)\n",
    "            'registration_year',\n",
    "            'registration_month',\n",
    "            'registration_day',\n",
    "            'registration_hour',\n",
    "            'registration_minute',\n",
    "            'withdrawl_year',\n",
    "            'withdrawl_month',\n",
    "            'withdrawl_day',\n",
    "            'withdrawl_hour',\n",
    "            'withdrawl_minute',\n",
    "            'completion_year',\n",
    "            'completion_month',\n",
    "            'completion_day',\n",
    "            'completion_hour',\n",
    "            'completion_minute',\n",
    "            'upgrade_year',\n",
    "            'upgrade_month',\n",
    "            'upgrade_day',\n",
    "            'upgrade_hour',\n",
    "            'upgrade_minute'\n",
    "        ], axis=1)\n",
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "withdrew = pd.Series((BD['withdrawl_day'].map(int) != 0).map(int), name=\"withdrew\")\n",
    "# completed = pd.Series((BD['completion_day'].map(int) != 0).map(int), name=\"completed\")\n",
    "\n",
    "DF = pd.concat([DF, withdrew], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>completed</th>\n",
       "      <th>upgraded</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>job</th>\n",
       "      <th>major</th>\n",
       "      <th>logged_location</th>\n",
       "      <th>withdrew</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2011_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24995</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>2013_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24996</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2013_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24997</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>2013_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24998</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2013_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24999</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2013_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       course  completed  upgraded  gender  country  age  education  job  \\\n",
       "0           1          0         0       0        0    0          0    0   \n",
       "1           1          0         0       0        0    0          0    0   \n",
       "2           1          0         0       0        0    0          0    0   \n",
       "3           1          0         0       0        0    0          0    0   \n",
       "4           1          1         0       0        0    0          0    0   \n",
       "...       ...        ...       ...     ...      ...  ...        ...  ...   \n",
       "24995       3          0         0       0        0    0          0    0   \n",
       "24996       3          0         0       0        0    0          0    0   \n",
       "24997       3          0         0       0        0    0          0    0   \n",
       "24998       3          0         0       0        0    0          0    0   \n",
       "24999       3          0         0       0        0    0          0    0   \n",
       "\n",
       "       major  logged_location  withdrew   index  \n",
       "0          0                0         1  2011_2  \n",
       "1          0                0         1  2011_3  \n",
       "2          0                0         1  2011_2  \n",
       "3          0               56         1  2011_4  \n",
       "4          0                0         0  2011_2  \n",
       "...      ...              ...       ...     ...  \n",
       "24995      0               56         0  2013_7  \n",
       "24996      0               56         1  2013_3  \n",
       "24997      0              171         1  2013_4  \n",
       "24998      0               56         1  2013_5  \n",
       "24999      0               51         1  2013_5  \n",
       "\n",
       "[25000 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# day_of_reg = pd.Series(BD['registration_year'].map(str) + \"-\" + BD['registration_month'].map(str) + \"-\" + BD['registration_day'].map(str), name=\"DoR\")\n",
    "# day_of_with = pd.Series(BD['withdrawl_year'].map(str) + \"-\" + BD['withdrawl_month'].map(str) + \"-\" + BD['withdrawl_day'].map(str), name=\"DoW\")\n",
    "# day_of_com = pd.Series(BD['completion_year'].map(str) + \"-\" + BD['completion_month'].map(str) + \"-\" + BD['completion_day'].map(str), name=\"DoC\")\n",
    "# day_of_upg = pd.Series(BD['upgrade_year'].map(str) + \"-\" + BD['upgrade_month'].map(str) + \"-\" + BD['upgrade_day'].map(str), name=\"DoU\")\n",
    "\n",
    "reg_month = pd.Series(BD['registration_year'].map(str) + \"_\" + BD['registration_month'].map(str), name=\"index\")\n",
    "\n",
    "# DF = pd.concat([DF, day_of_reg, day_of_with, day_of_com, day_of_upg], axis=1)\n",
    "DF = pd.concat([DF, reg_month], axis=1)\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kusalh\\AppData\\Local\\Continuum\\miniconda3\\envs\\course\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'registration': {'2010_1': 0,\n",
       "  '2010_2': 0,\n",
       "  '2010_3': 0,\n",
       "  '2010_4': 0,\n",
       "  '2010_5': 0,\n",
       "  '2010_6': 0,\n",
       "  '2010_7': 0,\n",
       "  '2010_8': 0,\n",
       "  '2010_11': 0,\n",
       "  '2010_12': 1,\n",
       "  '2011_1': 0,\n",
       "  '2011_2': 3198,\n",
       "  '2011_3': 2005,\n",
       "  '2011_4': 3320,\n",
       "  '2011_5': 1013,\n",
       "  '2011_6': 459,\n",
       "  '2011_7': 1,\n",
       "  '2011_8': 0,\n",
       "  '2011_11': 2,\n",
       "  '2011_12': 1249,\n",
       "  '2012_1': 1871,\n",
       "  '2012_2': 2161,\n",
       "  '2012_3': 3820,\n",
       "  '2012_4': 703,\n",
       "  '2012_5': 191,\n",
       "  '2012_6': 2,\n",
       "  '2012_7': 0,\n",
       "  '2012_8': 1,\n",
       "  '2012_11': 0,\n",
       "  '2012_12': 0,\n",
       "  '2013_1': 0,\n",
       "  '2013_2': 2,\n",
       "  '2013_3': 694,\n",
       "  '2013_4': 338,\n",
       "  '2013_5': 2863,\n",
       "  '2013_6': 671,\n",
       "  '2013_7': 416,\n",
       "  '2013_8': 19,\n",
       "  '2013_11': 0,\n",
       "  '2013_12': 0},\n",
       " 'withdrawl': {'2011_1': 0,\n",
       "  '2011_2': 14,\n",
       "  '2011_3': 33,\n",
       "  '2011_4': 194,\n",
       "  '2011_5': 299,\n",
       "  '2011_6': 207,\n",
       "  '2011_7': 88,\n",
       "  '2011_8': 75,\n",
       "  '2011_9': 88,\n",
       "  '2011_10': 64,\n",
       "  '2011_11': 35,\n",
       "  '2011_12': 24,\n",
       "  '2012_1': 69,\n",
       "  '2012_2': 95,\n",
       "  '2012_3': 362,\n",
       "  '2012_4': 277,\n",
       "  '2012_5': 175,\n",
       "  '2012_6': 105,\n",
       "  '2012_7': 70,\n",
       "  '2012_8': 106,\n",
       "  '2012_9': 92,\n",
       "  '2012_10': 82,\n",
       "  '2012_11': 46,\n",
       "  '2012_12': 37,\n",
       "  '2013_1': 43,\n",
       "  '2013_2': 34,\n",
       "  '2013_3': 43,\n",
       "  '2013_4': 31,\n",
       "  '2013_5': 124,\n",
       "  '2013_6': 135,\n",
       "  '2013_7': 57,\n",
       "  '2013_8': 11,\n",
       "  '2013_9': 0,\n",
       "  '2013_10': 0,\n",
       "  '2013_11': 0,\n",
       "  '2013_12': 0},\n",
       " 'completion': {'2011_1': 0,\n",
       "  '2011_2': 0,\n",
       "  '2011_3': 0,\n",
       "  '2011_4': 22,\n",
       "  '2011_5': 570,\n",
       "  '2011_6': 414,\n",
       "  '2011_7': 63,\n",
       "  '2011_8': 29,\n",
       "  '2011_9': 13,\n",
       "  '2011_10': 8,\n",
       "  '2011_11': 6,\n",
       "  '2011_12': 3,\n",
       "  '2012_1': 5,\n",
       "  '2012_2': 3,\n",
       "  '2012_3': 59,\n",
       "  '2012_4': 386,\n",
       "  '2012_5': 71,\n",
       "  '2012_6': 17,\n",
       "  '2012_7': 10,\n",
       "  '2012_8': 7,\n",
       "  '2012_9': 3,\n",
       "  '2012_10': 9,\n",
       "  '2012_11': 2,\n",
       "  '2012_12': 2,\n",
       "  '2013_1': 2,\n",
       "  '2013_2': 5,\n",
       "  '2013_3': 2,\n",
       "  '2013_4': 3,\n",
       "  '2013_5': 20,\n",
       "  '2013_6': 150,\n",
       "  '2013_7': 59,\n",
       "  '2013_8': 1,\n",
       "  '2013_9': 0,\n",
       "  '2013_10': 0,\n",
       "  '2013_11': 0,\n",
       "  '2013_12': 0},\n",
       " 'upgrade': {'2011_1': 0,\n",
       "  '2011_2': 0,\n",
       "  '2011_3': 0,\n",
       "  '2011_4': 2,\n",
       "  '2011_5': 14,\n",
       "  '2011_6': 53,\n",
       "  '2011_7': 15,\n",
       "  '2011_8': 4,\n",
       "  '2011_9': 5,\n",
       "  '2011_10': 5,\n",
       "  '2011_11': 4,\n",
       "  '2011_12': 2,\n",
       "  '2012_1': 2,\n",
       "  '2012_2': 1,\n",
       "  '2012_3': 2,\n",
       "  '2012_4': 8,\n",
       "  '2012_5': 20,\n",
       "  '2012_6': 4,\n",
       "  '2012_7': 2,\n",
       "  '2012_8': 3,\n",
       "  '2012_9': 2,\n",
       "  '2012_10': 1,\n",
       "  '2012_11': 3,\n",
       "  '2012_12': 0,\n",
       "  '2013_1': 0,\n",
       "  '2013_2': 1,\n",
       "  '2013_3': 1,\n",
       "  '2013_4': 0,\n",
       "  '2013_5': 11,\n",
       "  '2013_6': 11,\n",
       "  '2013_7': 24,\n",
       "  '2013_8': 9,\n",
       "  '2013_9': 0,\n",
       "  '2013_10': 0,\n",
       "  '2013_11': 0,\n",
       "  '2013_12': 0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phases = {'registration': day_of_reg, 'withdrawl':day_of_with, 'completion':day_of_com}\n",
    "phases = ['registration', 'withdrawl', 'completion', 'upgrade']\n",
    "buckets = {}\n",
    "\n",
    "for phase in phases:\n",
    "    y = phase + '_year'\n",
    "    m = phase + '_month'\n",
    "    \n",
    "    years = BD[y][BD[y] != 0].unique()\n",
    "    years.sort()\n",
    "    months = BD[m][BD[m] != 0].unique()\n",
    "    months.sort()\n",
    "    bucket = {}\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            count = len(BD[BD[phase + '_year'] == year][BD[phase + '_month'] == month])\n",
    "            bucket[str(year) + \"_\" + str(month)] = count\n",
    "    buckets[phase] = bucket\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>registration</th>\n",
       "      <th>withdrawl</th>\n",
       "      <th>completion</th>\n",
       "      <th>upgrade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2011_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2011_2</td>\n",
       "      <td>3198.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2011_3</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2011_4</td>\n",
       "      <td>3320.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2011_5</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2011_6</td>\n",
       "      <td>459.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2011_7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2011_8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2011_11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2011_12</td>\n",
       "      <td>1249.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2012_1</td>\n",
       "      <td>1871.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2012_2</td>\n",
       "      <td>2161.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2012_3</td>\n",
       "      <td>3820.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2012_4</td>\n",
       "      <td>703.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2012_5</td>\n",
       "      <td>191.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2012_6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2012_7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2012_8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2012_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2012_12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2013_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2013_2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2013_3</td>\n",
       "      <td>694.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2013_4</td>\n",
       "      <td>338.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2013_5</td>\n",
       "      <td>2863.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2013_6</td>\n",
       "      <td>671.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2013_7</td>\n",
       "      <td>416.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2013_8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2013_11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2013_12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  registration  withdrawl  completion  upgrade\n",
       "10   2011_1           0.0        0.0         0.0      0.0\n",
       "11   2011_2        3198.0       14.0         0.0      0.0\n",
       "12   2011_3        2005.0       33.0         0.0      0.0\n",
       "13   2011_4        3320.0      194.0        22.0      2.0\n",
       "14   2011_5        1013.0      299.0       570.0     14.0\n",
       "15   2011_6         459.0      207.0       414.0     53.0\n",
       "16   2011_7           1.0       88.0        63.0     15.0\n",
       "17   2011_8           0.0       75.0        29.0      4.0\n",
       "18  2011_11           2.0       35.0         6.0      4.0\n",
       "19  2011_12        1249.0       24.0         3.0      2.0\n",
       "20   2012_1        1871.0       69.0         5.0      2.0\n",
       "21   2012_2        2161.0       95.0         3.0      1.0\n",
       "22   2012_3        3820.0      362.0        59.0      2.0\n",
       "23   2012_4         703.0      277.0       386.0      8.0\n",
       "24   2012_5         191.0      175.0        71.0     20.0\n",
       "25   2012_6           2.0      105.0        17.0      4.0\n",
       "26   2012_7           0.0       70.0        10.0      2.0\n",
       "27   2012_8           1.0      106.0         7.0      3.0\n",
       "28  2012_11           0.0       46.0         2.0      3.0\n",
       "29  2012_12           0.0       37.0         2.0      0.0\n",
       "30   2013_1           0.0       43.0         2.0      0.0\n",
       "31   2013_2           2.0       34.0         5.0      1.0\n",
       "32   2013_3         694.0       43.0         2.0      1.0\n",
       "33   2013_4         338.0       31.0         3.0      0.0\n",
       "34   2013_5        2863.0      124.0        20.0     11.0\n",
       "35   2013_6         671.0      135.0       150.0     11.0\n",
       "36   2013_7         416.0       57.0        59.0     24.0\n",
       "37   2013_8          19.0       11.0         1.0      9.0\n",
       "38  2013_11           0.0        0.0         0.0      0.0\n",
       "39  2013_12           0.0        0.0         0.0      0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates = pd.DataFrame(buckets).reset_index()\n",
    "rates = rates.dropna()\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inf rate is interpretted as 100% and NaN rates caused by division by zero interpretted as 0%\n",
    "rates['subscribers'] = rates['registration'].cumsum() - rates['withdrawl'].cumsum()\n",
    "rates['subscribe_rate'] = rates['registration'] * 100 / (rates['subscribers'] + rates['withdrawl'] - rates['registration'])\n",
    "rates['churn_rate'] = rates['withdrawl'] * 100 / (rates['subscribers'] + rates['withdrawl'] - rates['registration'])\n",
    "rates['upgrade_rate'] = rates['upgrade'] * 100 / (rates['subscribers'] + rates['withdrawl'] - rates['registration'])\n",
    "rates['completion_rate'] = rates['completion'] * 100 / (rates['subscribers'] + rates['withdrawl'] - rates['registration'])\n",
    "rates = rates.replace(np.nan, 0)\n",
    "rates = rates.replace(np.inf, 100)\n",
    "rates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>completed</th>\n",
       "      <th>upgraded</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>job</th>\n",
       "      <th>major</th>\n",
       "      <th>logged_location</th>\n",
       "      <th>withdrew</th>\n",
       "      <th>registration</th>\n",
       "      <th>withdrawl</th>\n",
       "      <th>completion</th>\n",
       "      <th>upgrade</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>subscribe_rate</th>\n",
       "      <th>churn_rate</th>\n",
       "      <th>upgrade_rate</th>\n",
       "      <th>completion_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3198.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5156.0</td>\n",
       "      <td>62.971106</td>\n",
       "      <td>1.036432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3198.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>3320.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8282.0</td>\n",
       "      <td>64.391001</td>\n",
       "      <td>3.762607</td>\n",
       "      <td>0.038790</td>\n",
       "      <td>0.426687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3198.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24995</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22202.0</td>\n",
       "      <td>1.904500</td>\n",
       "      <td>0.260953</td>\n",
       "      <td>0.109875</td>\n",
       "      <td>0.270109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24996</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>694.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18261.0</td>\n",
       "      <td>3.940943</td>\n",
       "      <td>0.244179</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.011357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24997</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>338.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18568.0</td>\n",
       "      <td>1.850939</td>\n",
       "      <td>0.169761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24998</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2863.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21307.0</td>\n",
       "      <td>15.419000</td>\n",
       "      <td>0.667816</td>\n",
       "      <td>0.059242</td>\n",
       "      <td>0.107712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24999</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2863.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21307.0</td>\n",
       "      <td>15.419000</td>\n",
       "      <td>0.667816</td>\n",
       "      <td>0.059242</td>\n",
       "      <td>0.107712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       course  completed  upgraded  gender  country  age  education  job  \\\n",
       "0           1          0         0       0        0    0          0    0   \n",
       "1           1          0         0       0        0    0          0    0   \n",
       "2           1          0         0       0        0    0          0    0   \n",
       "3           1          0         0       0        0    0          0    0   \n",
       "4           1          1         0       0        0    0          0    0   \n",
       "...       ...        ...       ...     ...      ...  ...        ...  ...   \n",
       "24995       3          0         0       0        0    0          0    0   \n",
       "24996       3          0         0       0        0    0          0    0   \n",
       "24997       3          0         0       0        0    0          0    0   \n",
       "24998       3          0         0       0        0    0          0    0   \n",
       "24999       3          0         0       0        0    0          0    0   \n",
       "\n",
       "       major  logged_location  withdrew  registration  withdrawl  completion  \\\n",
       "0          0                0         1        3198.0       14.0         0.0   \n",
       "1          0                0         1        2005.0       33.0         0.0   \n",
       "2          0                0         1        3198.0       14.0         0.0   \n",
       "3          0               56         1        3320.0      194.0        22.0   \n",
       "4          0                0         0        3198.0       14.0         0.0   \n",
       "...      ...              ...       ...           ...        ...         ...   \n",
       "24995      0               56         0         416.0       57.0        59.0   \n",
       "24996      0               56         1         694.0       43.0         2.0   \n",
       "24997      0              171         1         338.0       31.0         3.0   \n",
       "24998      0               56         1        2863.0      124.0        20.0   \n",
       "24999      0               51         1        2863.0      124.0        20.0   \n",
       "\n",
       "       upgrade  subscribers  subscribe_rate  churn_rate  upgrade_rate  \\\n",
       "0          0.0       3184.0      100.000000  100.000000      0.000000   \n",
       "1          0.0       5156.0       62.971106    1.036432      0.000000   \n",
       "2          0.0       3184.0      100.000000  100.000000      0.000000   \n",
       "3          2.0       8282.0       64.391001    3.762607      0.038790   \n",
       "4          0.0       3184.0      100.000000  100.000000      0.000000   \n",
       "...        ...          ...             ...         ...           ...   \n",
       "24995     24.0      22202.0        1.904500    0.260953      0.109875   \n",
       "24996      1.0      18261.0        3.940943    0.244179      0.005679   \n",
       "24997      0.0      18568.0        1.850939    0.169761      0.000000   \n",
       "24998     11.0      21307.0       15.419000    0.667816      0.059242   \n",
       "24999     11.0      21307.0       15.419000    0.667816      0.059242   \n",
       "\n",
       "       completion_rate  \n",
       "0             0.000000  \n",
       "1             0.000000  \n",
       "2             0.000000  \n",
       "3             0.426687  \n",
       "4             0.000000  \n",
       "...                ...  \n",
       "24995         0.270109  \n",
       "24996         0.011357  \n",
       "24997         0.016428  \n",
       "24998         0.107712  \n",
       "24999         0.107712  \n",
       "\n",
       "[25000 rows x 20 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = DF.merge(rates, how=\"left\", on=\"index\")\n",
    "t = t.drop(['index'], axis=1)\n",
    "# len(DF.columns), len(t)\n",
    "# t, len(t.columns)\n",
    "# DF.columns\n",
    "# t.shape\n",
    "DF = t.fillna(0)\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'course'),\n",
       " Text(1.5, 0, 'completed'),\n",
       " Text(2.5, 0, 'upgraded'),\n",
       " Text(3.5, 0, 'gender'),\n",
       " Text(4.5, 0, 'country'),\n",
       " Text(5.5, 0, 'age'),\n",
       " Text(6.5, 0, 'education'),\n",
       " Text(7.5, 0, 'job'),\n",
       " Text(8.5, 0, 'major'),\n",
       " Text(9.5, 0, 'logged_location'),\n",
       " Text(10.5, 0, 'withdrew'),\n",
       " Text(11.5, 0, 'registration'),\n",
       " Text(12.5, 0, 'withdrawl'),\n",
       " Text(13.5, 0, 'completion'),\n",
       " Text(14.5, 0, 'subscribers'),\n",
       " Text(15.5, 0, 'subscribe_rate'),\n",
       " Text(16.5, 0, 'churn_rate'),\n",
       " Text(17.5, 0, 'upgrade_rate'),\n",
       " Text(18.5, 0, 'completion_rate')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEzCAYAAAA1h6QjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd7gU1fnHP997qQKCICpWLFgBUbCgqNh7L1hiT0w0ajRRoz9LiCXRGKMxxoJKUezYsMQSFLArSLNrBBN7DQFBhHvf3x/nLHdYdu/ucNveve/neebZmTPvnHNmdve8c9r3yMxwHMdxnAwVTZ0Bx3Ecp7Rwx+A4juMsgTsGx3EcZwncMTiO4zhL4I7BcRzHWQJ3DI7jOM4SuGNwHMdxlsAdQwokVTZ1HhzHcRqasncMko6RNF3SNEm3S1pL0rgYNk7SmtFupKRDEtfNjZ+DJT0r6U5ghqQOkh6L8b0haUi06y9pgqTJkp6U1KNJbthxHKeOtGrqDDQkkjYBzge2NbOvJXUFRgG3mdkoSScA1wIHFIhqS6C3mc2UdDDwqZntHdPoLKk18DdgfzP7KjqLy4ATGujWHMdxGoyydgzATsAYM/sawMy+lTQQOCievx34UxHxvGpmM+P+DODPkq4AHjWz5yT1BnoDT0sCqAQ+yxWRpJOAkwB67XN4/1X7b1v0zfzjpH2LtgVY1KlrKvuFla1T2VdVp5NTqaxQKvu0tK5amMo+7f1e+/iEVPbnbL1BKvsFX+X8yeSl1Xq9U9lXfPN5KnuAI+5Nd8+Xfj85lX3XzYv//QOsOHDnVPbzP/0olf0K/bau84908NDriv5jjB96asP+KZaRcm9KElDoS8qcX0R8Hgqle5uEzfeLjc3eA/oTHMQfJV0U03nTzPrFrY+Z7ZYzMbNhZjbAzAakcQqO4ziNRbk7hnHAYZK6AcSmpBeBw+P5o4Dn4/4sQoEPsD+Q83VS0qrAPDMbDfwZ2Bx4F+geayNIah2bsRzHaWFIKnorVcq6KcnM3pR0GTBBUhUwBTgdGC7pbOAr4PhofjPwsKRXCQ7l+1xxAn2AKyVVAwuBk83sx9hxfa2kzoTneg3wZkPdm+M4pUlDN5k2BmXtGADMbBShwznJTjnsvgC2TgSdF8PHA+MTdk8CT+a4fiqwfZ0z7DiO08TI12NoOuZ/+lGqh7/nsEdSxX/pEfuksl+z+wqp7Nu1Ttd5u3BRVSr7KqtOZZ+2at6mMt20lOW//zaVfUXK5/O/dp1T2bezRansl4XK2d+ksv+mQ7oBD2l/Qw19z506darz6/7ul91Y9P/6yfN/UZLVi7KvMTiO4zQmpdx3UCzuGBzHceqRijJwDOU+KqlWsmc757E5Lo5EShNvT0lv1C13juM0RyoqVPRWqrRox1AkxwGpHIPjOC2XCqnorVQpSceQUt/ohqhl9KGkHSQNl/S2pJGJ+OZKukrS6/H67jnSXErrKNYmBgB3SJoqqX0+TaQYPk3SS8AvG+dJOY5TarhjaAAS+kY7mdmmwK+A6wj6Rn2BOwj6RhlWIAw/PRN4BLga2AToI6lftOkAvG5mmwMTgN9lpZnROjrEzPoDw4HLzGwMMAk4ysz6EWZHL2UXoxkBnG5mA+vtYTiO0+wohwluJecYyKFvBAwE7oznbwcGJewfsTDmdgbwhZnNMLNqwuSyntGmGrgn7o/Ouh5gA2q0jqYCFwCr58hbTrs4qa2LmWWEZW7Pd3OSTpI0SdKkW0ffmc/McRynySjFUUlp9I0AFsTP6sR+5jjf/WXHn9E6KvS2n9NOUpccceZO2GwYMAzSz2NwHKf0KYeZz6VYY0ijb1QsFUBm9NGROa6vTetoDtCpNjsz+y8wW1KmJnJUyvw5jlMmVFZUFL2VKiVXY0ipb1Qs3wObSJoMzAaGZKVZm9bRSOBGSfMJTVr57I6PeZxHDskMx3FaBhUq3QK/WErOMUAqfaPjEvuzCO3/S52LxxcCF9ZyfU6tIzO7H7g/EZTPbjKwaSJoaLaN4zhOc6AkHUNLIe1COmm1jy6469FU9o/85thU9pBO+6h96l9bQ795pdNiWtSxSwPlI9AY2kdpqercLZV96idUgvdcV8qgi6Ek+xjqHTPr2NR5cBynZVDfw1Ul7SHpXUkfSDo3x/k141yuKXGu1151vQevMTiO49Qj9Sl1IakS+DuwK/Ax8JqksWb2VsLsAuBeM7tB0sbA49QM1V8mWkSNoa5IGiwpVbuMpPGSBjRUnhzHKU0qVVH0VgRbAh+Y2Ydm9iNwN2GFySQGLB/3OwOf1vUeWmyNQaEepzgZznEcp15IM6NZ0knASYmgYXGuU4bVgP8kjj8GtsqKZijwlKTTCCoPu6TJby6ahWOQ1BN41Mx6x+OzgI7AYMIooS0JHvMEM3s1aiHdCXQDXgP2IKzn3BH4B/AsYejpAbHNbgugPWHG9e9iGnsQhqJ+DbyeyEsHgixGH8LzG2pmD0tqT5DF2Bh4O8bnOE4LI01LUnLCax5yxZY9MfYIYKSZXRXnWN0uqXddXnrLoSmpg5ltA5xC0C6CoIX0TNRGehBYM2G/AUF3aTMz+wg438wGAH2BHST1ldSOsAb0vsB2wCqJ68+PcW8B7EhY/7kDcDIwL+o5XUZwRI7jtDAqKiqK3orgY2CNxPHqLN1UdCJwL4CZvQS0A1as0z3U5eIS4S4AM5sILB/lKQYR2uIwsyeA7xL2H5nZy4njwyS9TphItwnhjX9DYKaZvR91mEYn7HcDzo1aSeMJX8KahLkNo2Oa04HpuTKb1EoaMWJEnW7ccZyy5zWgl6S1JbUhKECMzbL5N7AzgKSNCGXSV3VJtFk0JRFUTZNOrF1iP7taZeSufmX4PrMjaW3gLGALM/suSnVn4s6nYyTgYDN7d4nA0K5YUPsoWXWcM2eOayU5TplRn3LaZrZI0qkENYVKYHhUh7gYmGRmY4HfADdLOpNQBh0XX2iXmeZSY/gCWElSN0ltgeRMryEAUadotpnNJmghHRbDdyNIc+dieYKjmC1pZWDPGP4OsLakdePxEYlrngROi53XSNoshk8kaiRJ6k1omnIcp4VR3yu4mdnjZra+ma1rZpfFsIuiU8DM3jKzbc1sUzPrZ2ZP1fUemkWNwcwWRg/5CjCTUHBn+E7Si8TO5xj2e+AuSUMI6y98RhDDW2Kim5lNkzSFoHX0IfBCDP8hjhZ4TNLXBEeTkdu4hNApPT06h1kER3UDMELSdEKH+Kv19wQcx2kutCphcbxiaRaOAcDMrmXJBXqQNB6438zOyzKfDeweq2EDgR3NbAGhEO+dNMzWVEqEP0Hoa8gOnw/8PE/44dnhjuM4zY1m4xhSsiZwr6QK4EfgZ02cn5wsrGydyn7N7vlaxHKTVvto36uydQtrZ8PVVk5ln3ZG6Kffzk5l375Nm1T23ZfvkMp+6JB0SgMLF6XTklpODT+lpu2iBYWNEixo1baBcrJspM2/Vad9pp0KmxSglFdmK5Zm7RjMbHCe8PeBzXKdc5yWStpC1Vk2Snkt52Jp1o7BcRyn1GhV2fz7GJr/HTQikkbGBX0cx3HKFq8xNCCSWpmVoeC84zh58T6GEkbShYR5Bf8h6B1NJshj/B3oDswDfmZm78SJbf8DBhDkL84xszFxOOrfCKvHzSQxcU5Sf+AvhCGwXxMmlXwWR0q9CGxLmKF4VYPfrOM4JYM7hhIlyl0fTOiAbkUQwZtMmHH8CzN7X9JWwPXULBnagyClsSGhQB8DHEjQVuoDrAy8RVjXuTXBYexvZl/F+RKXUTOPoouZ7dDgN+o4Tsnh8xhKl0HAw3FuAZIeIUhdbAPcl/DoybF4D0U1wrfiLGgI+kd3mVkV8KmkZ2L4BoT5EE/HuCoJk+gy3JMvY0mZ3b/87TqOPeHEZb5Jx3GchqBcHUOuulwF8F8z65fnmuRYvuT1uTRHBLxpZgPzxPV9nvAltJK+nfeDayU5TplRnyu4NRXNv86Tm+eBfSW1k9QR2JvQpzBT0qEQFuqRtGmBeCYCh0uqlNSDILMN8C7QPc6qRlJrSZs0yJ04jtOsqKyoKHorVcqyxmBmr0kaC0wDPgImEWQyjgJukHQB0JogzT2tlqgeJPRBzADeI+guYWY/xmGr10rqTHiO1xA0lxzHacH4BLfS5s9mNlTScoQ3/6vMbCZhNbclyNZLMrOO8dOAU3NFbmZTCX0Q2eGD65xzx3GcJqScHcMwSRsTOp1HmdnrhS5obKqq03UxtGudTlsJ0mn1pNU+eueTL1LZb7vhOqns5y9YmMq+U/t0uj7t26bTVvp2zrxU9p07pFzdtYGXH18W3aMflK6IaNfA03bS3kNTyID4cNUSxsyObOo8OI7T8mj+bqGMHYPjOE5TUMqdysXijsFxHKce8eGqLQhJZ8SObMdxnLxIKnorVdwxFM8ZQE7HIKmykfPiOE6JUikVvZUqZeUYJB0jabqkaZJul7SWpHExbJykNaPdEvLZkubGz8GSxksaI+kdSXfEiXCnA6sCz0p6NnONpIslvQJcIOnBRHy7SnqgUW/ecZySoEIqeitVysYxxJnH5wM7mdmmwK+A64DbzKwvcAdZa0bnYTNC7WBjYB1g27je9KeEtaMzs587AG+Y2VbAxcBGkrrHc8cDI/Lk8yRJkyRNum34rctyq47jlDDl0JRUTp3POwFjzOxrADP7NkpWHBTP3w78qYh4XjWzjwEkTQV6EiQ2sqkC7o9pmaTbgZ9IGgEMBI7JFXlSK+mrufNdK8lxyoxSLvCLpZwcg8gteJckc34RsbYU11xIznRKzoipIv8z+iGqrmYYATwC/ADc5wv0OE7LpNJHJZUU44DDJHUDkNSVsGDO4fH8UdS8+c8C+sf9/Qm6SYWYA3TKd9LMPiU0N10AjEyXdcdxygVvSiohzOxNSZcBEyRVAVOA0wkL65wNfEVo+we4GXhY0qsEh5JXJjvBMOAfkj5L9DNkcwfQ3czeqsu9OI7jNCVl4xgAzGwUMCoreKccdl8AWyeCzovh44HxCbtTE/t/I6zaljnumCMLgwhOpyjSVjnnL0jXOtU+5bebdmJOWu2jF975MJV937VWTWXftVOHVPbLpdRK6top3TSWRVUNq320LKTWDiqrEqJxKOXRRsXiX3s9IWkyoebxm6bOi+PkoikE5VoirSub/7Qmdwz1hJn1L2zlOI5T+rhjcBzHqUdKuVO5WNwxOI7j1CNl4BfcMTiO49Qn5SC73fzvoAGR9JCkyZLelHRSDDtR0ntRU+lmSdfF8O6S7pf0Wty2bdrcO47TFJTDPAZ3DLVzQuxUHgCcLmk14ELCUNddgQ0Ttn8FrjazLYCDgVtyRZjUShrlWkmOU3ZUqPitVPGmpNo5XdKBcX8N4Ghggpl9CyDpPmD9eH4XYOPEW8DykjqZ2ZxkhEmtpG/n/eBaSY5TZtR3U5KkPQgvnpXALWZ2eR67Q4D7gC3MbFJd0nTHkAdJgwmF/UAzmydpPPAusFGeSyqi7fzGyaHjOKVIfU5wi2u9/J3QQvEx8JqksdnqCpI6EZQeXqmPdL0pKT+dge+iU9iQ0Hy0HLCDpBUktSI0GWV4Clg8U1pSv0bNreM4JUE99zFsCXxgZh+a2Y/A3QR9t2wuIahH/1Af9+COIT9PAK0kTSc89JeBT4A/ELzyP4G3gNnR/nRgQFwU6C3gF42fZcdxyozVgP8kjj+OYYuRtBmwhpk9Wl+JelNSHsxsAbBndrikSWY2LNYYHiTUFIjrQAxpyDxVWVrtnXR+/9NvZxc2SjB/wcJU9mm1j6Z/9Gkq+xU6ptMy6r58Lrmr/Jw7eiyXHblv0faSqK4u/jubRwXLqeH0lRa0atvsZTHS5t9SPP/6olVl8f+7ONrxpETQsNgPudgkx2WL+yYlVQBXA8ely2XtuGNIz1BJuwDtCE7hoSbOj9NIpHEKQCqnADSoUwDXSmos0gxDTQ5GycPHhIEvGVYnyPtn6AT0BsbHdFcBxkrary4d0O4YUmJmZzV1HhzHaTG8BvSStDahKftw4MjMSTObDayYOY6DZM7yUUmO4zglREXO1p9lw8wWSToVeJIwXHV4XHvmYmCSmY2tt8QSNHvHIOk4YEBy7YR6iPMA4L3MkLD4JUw0s3/WVxqO45QnFfU8j8HMHgcezwq7KI/t4PpIs9k7hgbiAOBRwqijvF+C4zhONmkXvCpFSn64qqSfSHpV0lRJN0mqlHR81CuaAGybsB0ZZ/9ljucm9s+RNEPSNEmXx7CfRV2jaVHnaDlJ2wD7AVfGNNdNxitpZ0lTYlzDJbWN4bMk/V7S6/FcUi7DcZwWQjlIYpS0Y5C0EWEI6LZm1g+oAn4C/J7gEHYFNi4inj0JtYCtzGxTwkQQgAfMbIsY9jZwopm9CIwFzjazfmb2r0Q87YCRwBAz60OocZ2cSOprM9scuAHI2UntWkmOU96Ug4heqTcl7Qz0J0wDB2gPbAOMN7OvACTdQ41eUT52AUaY2TyAjNYR0FvSpUAXoCOhg6c2NgBmmtl78XgU8Evgmnj8QPycDByUKwLXSnKc8qZSJf2+XRSl7hgEjDKz8xYHhI7hA/PYLyLWghQ8SWa1d5GYFJJgJHCAmU2LndiDi8hPbWQGildR+s/WcZwGoJRrAsVS6q5tHHCIpJUAJHUFpgCDJXWT1Bo4NGE/i1DDgKAn0jruPwWcIGm5RDwQJod8FuM5KhHPnHgum3eAnpLWi8dHAxOW/fYcxyk3vI+hgYnDRS8AnoqaRU8DPYChwEsEvaLXE5fcTBC5exXYCvg+xvMEod9gkqSp1LT/X0jQPXqaUOhnuBs4O3Yyr5vIzw/A8cB9kmYA1cCN9XnPjuM0byoqKoreShWZeTN3U5G2j2HuD+kkDVZone6Hd/SN96aLv0M6baJO7dumsi81raSWKImxoFW676yhaWitpLbdVqrze/zEd2YW/b/efsO1S7Le4O3gTUjrqnQidA3ddtm+TZvCRgnSFvRdO3VIZZ+2oP9u7rx08ad0bJUp6/5mJfmfb9akdVRNoQ9Vn+sxNBWlW5dxHMdxmgSvMTiO49QjPiqphSHpxVrODZZUbwtlOI7TPKmsUNFbqeI1hhSY2TZNnQfHcUqbyhIebVQszf8OGhFJcxW4UtIbURMpuWrb8pIelPSWpBvj6kqO4zjNCi+40nMQ0A/YlCC1caWkHvHclsBvgD7AuuSQxUhqJY0YMaKRsuw4TmNRUaGit1LFm5LSMwi4y8yqgC+iwusWwP+AV83sQwBJd0XbMcmLk1pJc+bM8UkkjlNmlMNwVXcM6antW88u6L3gd5wWho9KaplMBIbEdSG6A9sDr8ZzW0paO/YtDAGeb6pMOo7jLCteY0iHAQ8CA4Fp8fgcM/s8LszzEnA5oY9hYrR1HKcF0bqysqmzUGfcMRSJpG7AtxbEpc6O22LMbDwwvvFz5jhOKVEOTUnuGIpA0qqEQv/P9RnvwsrWhY0StLGqlCmkExDrvnw6LaP2bdNpKy2X0j6tyF1a7aMPv/g6lX1Vdbouo9QFhPdIlQUlPNioaNwxFIGZfUrhVeIcx3HKAncMjuM49Ugpr7NQLM3/DhoRSQMkXdvU+XAcp3SpVEXRW6niNYYUmNkkYFKx9pJamdmiBsyS4zglRhn0Pbe8GoOknpLekXRL1Du6Q9Iukl6Q9L6kLeP2Ylza80VJG8RrFyuoSuoq6SFJ0yW9LKlvDB8qaZikp4DbmvBWHcdxlokW5xgi6wF/BfoCGwJHEuQrzgL+j7D+8/ZmthlwEfCHHHH8HphiZn3jNUkn0B/Y38yOzL4oqZU0avit9XhLjuOUAq0qK4reSpWW2pQ008xmAEh6ExhnZiZpBtAT6AyMktSLMIgw17jSQcDBAGb2jKRukjrHc2PNbH6uhJNaSWnXfHYcp/Qph3kMpeuyGpbkQrDVieNqgrO8BHjWzHoD+wLtcsSR69vPFPTf11M+HcdpZlSgordSpaU6hkJ0Bj6J+8flsZkIHAWh7wH42sz+1+A5cxynpHHZ7fLlT4SmpF8Dz2Sdy9QKhgIjJE0H5gHHNl72HMcpVVx2uxliZrOA3onj4/KcS850vjB+dgO+jbbfAvvniH9oPWbXcZxmRjn0MShowjmFkLQfoSZxgpm9WB9xDh3zZKqH/9ttN04V/6KOXVLZp9Vu+nbOvFT2XTul0zJKq02UdnH1tPEf+ud0o8ieOOXgVPZV89N1TX3evlsq+1Wr5qayB/jzS++ksj/im6mp7Dutt1E6+169CxslWPT9nFT2y2/Qp86l+ldz5xf9w+resX1JepEWV2NYVsxsLDC2qfPhOI7T0LhjcBzHqUfS1lxLER+V5DiOU4+0rlpY9FYMkvaQ9K6kDySdm+N8W0n3xPOvSOpZ13so2jFISt9AWU9IGi9pQC3nZ0lasR7TGyxpm8TxLyQdU1/xO47jFIOkSuDvwJ7AxsARkrI7G08EvjOz9YCrgSvqmq7XGHIzGFjsGMzsRjNz3SPHcRqbLYEPzOxDM/sRuJulR0PuD4yK+2OAnVXHoVGpHYMCV0YBuhmShsTwCknXS3pT0qOSHpd0SDy3VxSue17StQkhug6Shkt6LQrW7R/D20u6OwrU3QO0T5G/X8e8vSHpjET4MTG+aZJuj2H7xqrXFEn/lLRyrIb9AjhT0lRJ20VhvLPiNf2iaN50SQ9KWiGGj5d0haRXJb0nabs8+VuslTT56cfTPn7HccqIZHkQt5OyTFYD/pM4/jiG5bSJas6zCUPrl5ll6Xw+COgHbAqsCLwmaSKwLUFnqA+wEvA2MFxSO+AmgijdTEl3JeI6H3jGzE6Q1AV4VdI/gZ8D88ysb1Qtfb2YjEnqDxwPbEWQrHhF0gTgx5jWtmb2taSu8ZLnga2jTtJPgXPM7DeSbgTmmtmfY7w7J5K5DTjNzCZIuhj4HZBxQK3MbEtJe8XwXbLzmNRKSjtc1XGc8iJZHuShNumdNDapWBbHMAi4y8yqgC9iwbtFDL/PzKqBzyU9G+03BD40s5nx+C4g4xV3A/bLvI0TNInWBLYHrgUws+lxdnGxeXvQzL4HkPQAsB3hIY0xs69jnN9G+9WBeyT1ANoAM5eOsoYoktfFzCbEoFHAfQmTB+LnZIKTdBzHqQsfA2skjlcHPs1j87GkVgRJn2+pA8vSx5Cv7SpteObcwWbWL25rmtnb8dyyeLza8pArvr8B15lZH0ItJZdYXhoyYnxV+FBgx3HqzmtAL0lrS2oDHM7S86nGUiPJcwihFaZONYZlcQwTgSGSKiV1J7zdv0poljk49jWsTOjAhbC2wTqJIVRDEnE9CZyW6SiRtFkijYxAXW/CugnF5u0ASctJ6gAcCDwHjAMOk9QtxplpSkqK5SW1juYAnbIjN7PZwHeJ/oOjgQnZdo7jOPVB7DM4lVBWvg3ca2ZvSro4qjEA3Ap0k/QB8GtgqSGtaVmWt9oHgYHANMJb+Dlm9rmk+4GdgTeA94BXgNlmNl/SKcATkr4mOJEMlwDXANOjc5gF7APcQI1A3dSsa/JiZq9LGpmwv8XMpgBIugyYIKkKmEJQTR0K3CfpE+BlYO143SPAmNgZflpWMscCN0paDviQ0KfhOI4DQNtFCwobLWap98+lMLPHgcezwi5K7P8AHJoi0YLUq1aSpI5mNje+mb9K6Oz9PBEuwpjc983s6npLuJky7+NZqR5+Zbt0LV0/tu2QLj+WrgJZUZHOPu1vLa192hF6ae07zftvKvs9rr8/lf0jv0kn0JuuAIIFrdqmsgeo+PaLVPb/XX7lVPZtWqd7N23XwEuod+rUqc7Tlhd882XRP9y23VYqyWnS9d0O/mgcXdQGuMTMPo/hP5N0bAyfQhil5DiO45Qg9eoYzGxwnvCrCTPy6oSkV4Ds156jM8t0Oo7jNDVWVdXUWagzzWrms5ltlRjBlNkyazc/LqlL3E7JXKMgb/FoobiLtXMcx6kVqy5+K1GalWOoDTPby8z+C3QBTilkXyxxXLDjOE5RWHV10Vup0mwcg6RzJJ0e96+W9Ezc31nSaNUI6V0OrBvlLK6Ml3eUNEZBluOOxPDYPWLY84QZ3Zm0hkoaJukp4LY4NPdKBemO6ZJ+Hu2uzwwZi/IYw+P+iZIubaRH4ziOU680G8dAmKOQmT8wgFDYtybMdn4uYXcu8K/YzHR2DNuMIFuxMbAOsG2U6rgZ2DfGu0pWev2B/c3sSIJ64Wwz24Iwy/tnktbOytNqMX5y5GkxSW2U4XfcmfYZOI5T6nhTUqMyGegvqRNhhvFLBAexHXkK4QSvmtnHUa5jKkGuYkNgppm9H2cJjs66ZqyZzY/7uwHHSJpKmJ/RDegV091OQQb3LYJESA/CPI+cy3+a2TAzG2BmA0446sgUt+84TnPAqq3orVRpNu3nZrZQ0izChLIXgenAjsC6hBmBtZEc8J2Uq6jtm0kuwCuCcN6T2UYK6qp7EGoPXYHDCAJ86RabdRynLLCqhp1r0Rg0pxoDhML3rPj5HEEee2qWLkhOOYscvAOsLWndeHxELbZPAifHpiskrR8lNyDUXM5I5OksCtdgHMcpV8yK30qU5uYYngN6AC+Z2RfAD2QVwmb2DfCCwnoMV+aII2P3A0Hl9bHY+fxRLeneQmgqel3SG4QJeplax3MEue0PCPLgXbPz5DhOy8HMit5KlXqVxHDS4ZIY9Wvvkhi145IYhakPSYy5H75T9A+34zobtghJDCcFC776LJX9ol79Utmn/RMtp5SjJBp6VEVD/2VSvhNVzf++sFGCtAX9vleNKmyU4Df77VzYKMHmn0xNZQ/Qpe8Wqewfeq3YpVMCfdZcNZX9+h9PS2Wflk67HFDnOMph5rM7BsdxnPqkDFph3DE4juPUI+XQPN/cOp+XQNJ+kvIuSiGpX1x/OW28PSUdmTgeIOnaZc2n4zgtB6uuKnorVUrGMSiQKj9mNtbMLq/FpB+Q0zEU0EDqCSx2DGY2ycxOT5M3x3Gc5kqTOob4Zv62pOsJQz2PlvSSpNcl3SepY7TbK6NpJOnajAqqpOMkXRf3D41DVKdJmqiwPurFhGVIp0oakkMDqaek52J6r0vaJmbtcsKM5qmSzkwqr0rqKumhqJn0sso5Sr8AACAASURBVKS+MXyopOGSxkv6MKPr5DhOC6O6uvitRCmFGsMGwG3ArgRNol3MbHNgEvDrqGl0E7CnmQ0CuueJ5yJgdzPbFNjPzH6MYfdE3aR7ol1SA+lLYNeY3hAg01x0LvBcvC57HYnfA1PMrC/wfzHvGTYEdge2BH6XmRCXJKmVNPL+h4p7Qo7jNBvMqoveSpVS6Hz+yMxelrQPQYTuhTi+vA1hVvGGwIdmNjPa30WYmJbNC8BISfcCD9SSXlIDqTVwnaR+BKmM9YvI7yDgYAAze0ZSN0md47nHzGwBsEDSl8DKwMfJi81sGDAM4LspLzX/XirHcZbAFjV/SYxScAyZweECnjazJaQpJG1WTCRm9gtJWwF7A1NjYV9begBnAl8AmxJqTz8UkVSu0fWZAj6fJpPjOE6zoRSakjK8TJDDXg9A0nKS1idoGq0jqWe0G5LrYknrmtkrZnYR8DWwBoV1kzoDn0XV1aOByhhe23UTgaNimoOBr83sf8XcoOM4LYAy0EoqmTdaM/tK0nHAXZIyc/cvMLP3FJbqfELS18CreaK4UlIvwhv9OGAa8G/g3CiX/ccc11wP3C/pUOBZamoT04FFkqYBI4EpiWuGAiMkTQfmAemmtzqOU9aUct9BsTSpYzCzWUDvxPEzhIVwsnnWzDZU6Hz4O6FjGjMbSSi4MbODclz3bZ74Mum9D/RNBJ0XwxcC2XoD4+O5b4H9c8Q1NOu4d7aN4zjlj1W5Y2gsfibpWEKH9BTCKKVmT6v10vmOVg0sIObUzuftu6WyXyOlyF1a7aOrxo5LZX/CzgNT2QO0+yydPtRhHealsl84c1Iq+883GZTK/v3Pvkplf2gq6zx4jaFxiENGs4eNOo7jlBxWwvMTiqVZOAbHcZxmQwl3KhdLKY1KSo2kxyV1idspifDFM5ULXF+U3TLka259x+k4TvPAtZKaGDPby8z+C3QBTilkXywFdJQcx3HKmpJ2DJLOyWgOSbpa0jNxf2dJoyXNkrQiQdto3ahtlFnOs6OkMVFj6Y44oglJe2R0l4CDEmkVpaMk6XpJ+8X9ByUNj/snSrq0kR6N4zilimslNTgTge3i/gBCYd+aIEuRXFf5XOBfUdvo7Bi2GXAGQWZjHcLkuXbAzcC+Md5VstIrRkcpmafVYvzkyJPjOC2Q6qpFRW+lSqk7hslAf0mdCHITLxEcxHYULoRfNbOP46zmqQQp7Q2BmWb2voXVNEZnXZOto3SzpBnAfdQ4gOcIyqsbA28BX0jqAQwEXix0Q0kRvREjRhQydxynueEznxsWM1soaRZwPKHQnQ7sCKwLvF3g8ny6RbV9GwV1lMzsE0krAHsQag9dgcOAuWY2p4h7WiyiN2fOnNL9ZTiO02IpaccQmQicBZwAzAD+Akw2M4vdBlBYEynDO8DaUVfpX8ARtdh2Bj42s+o4ua4yce4lQjPVTkA3YEzcHMdp4ZTDPIZSb0qC0HTTA3jJzL4gvLkv0YxkZt8Q5LrfSHQ+L4WZ/UCQ7H4sdj5/VEu61wPHSnqZIMedrE08B7Qysw8ICwx1zc6T4zgtFKsufqsDcdGwpyW9Hz9XqMV2eUmfKC5sVoiSrzGY2ThCe3/meP3Efs/E/pFLXhm0jeK5UxP7TxD6GrLTGZp1nFNHKZ67Fbg17i8EOmRd27G2e3Icp3xpRK2kc4FxZna5pHPj8W/z2F4CTCg24pJ3DOVMxTefp7Kv7pY9iKp+aZtS22dBq7aFjeoQf6mxatX8wkYJFrTvksp+80+mprJPq300fNxLqewB7v3NCansqyf9J5V9p/U2LmyU4JbXZqSyn/jWB6nsD92qb2GjQjSeVtL+wOC4P4rwMryUY5DUn7Bo2BOEwTsFaQ5NSY7jOM0GMyt6qyMrm9lnMc3PgJWyDSRVAFcBZ2efqw2vMTiO49QjVlW81IWkk1hyqeJhceRi5vw/WXq+FcD5RSZxCvC4mf0nMVinIC3OMUgaCTxqZnlHEcUFg54ys0/j8S3AX8zsrUbJpOM4zZcUNYHk8PU853fJd07SF5J6mNlncS7VlznMBhLmXZ0CdATaSJprZufWlq8W5xiK5DjgDeBTADP7aZPmxnEcZ2nGElaQvDx+PpxtYGZHZfbjC++AQk4BSqiPQdIxkqZLmibpdklrSRoXw8ZJWjPajZR0g6RnJX0oaQdJwyW9HWsDmfjmSroq6hyNk9Q9R5r9JU2QNFnSk5J6SDqE0EFzR9Reai9pvKQB8ZojJM2IQ2OvyErvspj/lyWt3OAPzXGckqMR1VUvB3aV9D6wazxG0oDYyrHMlIRjkLQJoc1sJzPbFPgVcB1wm5n1Be6gRqsIYAXC5LIzgUcIi/hsAvSR1C/adABej1pHE4DfZaXZGvgbcIiZ9QeGA5fFJqZJwFFRe2l+4ppVgSti2v2ALSQdkEjv5Zj/icDP6v5kHMdpbljVoqK3OqVj9o2Z7WxmveLntzF8Uq5WDjMbmRy6Xxsl4RgIBe0YM/saFq+rPBC4M56/nSBSl+GRqHU0A/jCzGZETaQ3CZpIANXAPXF/dNb1ABsQ1pt+WtJU4AJg9QL53AIYb2ZfmdkigsPaPp77Ecis7TA5kY8lSGolDb/z7gLJOY7T3GjEUUkNRqn0MYjaNYzIOp8ZEF/NkppI1eS/p+z4BbxpZmkGg9fWrb/Qar7ppDbTkplIdDZ9P+v90v1lOI7TYimVGsM44DBJ3SBM9SaI5h0ezx8FPJ8yzgrgkLh/ZI7r3wW6SxoY02wdm7Qgv/bSK8AOklaUVEnQWip6NqHjOC2Aait+K1FKosZgZm9KugyYIKkKmAKcDgyXdDbwFUFhNQ3fA5tImgzMJqypkEzzx9jRfK2kzoRncQ2hOWokcKOk+YQmrcw1n0k6D3iWUHt43MyWGgngOE7LpZSX7CyWknAMAGY2ijCtO8lOOeyOS+zPIvQTLHUuHl8IXFjL9VOp6SNI2twP3J8IGpw4dyc1fR/Jazom9l1t1XFaKO4YnDpxxL3pWqHuPjLvXJecVHXulso+rfbRD2mXxm7mv7ZrXpqSyv7X/ddOZd+l7xap7Nt99n1howRpdY8ADrtqeCr7v8x5OZV9p3U3SmV/8jG/TmX/28H9Chs5S9HM/6r5cYVTx3GahBLuOyiWsnUMjuM4TUE5NCWVyqikgkgaKumseo5zgKRrGyp+x3FaHlZdXfRWqrTYGoOkVmY2iTDLua5xCVCcZOc4jtOsadIag6QOkh6L+kJvSBoiaZakFeP5AZLGJy7ZVNIzcSm7n0WbHpImRl2jNyRtF8P3iDpJ0ySNi2FDJQ2T9BRwm6TBkh6tLf543dmSXou6Tb+PYT2jPtP1hOU914g6Tm9ELaUzG/LZOY5TmjSWJEZD0tQ1hj2AT81sb4A4n+CKWuz7AlsTdImmSHqMMMnsSTO7LE46Wy4K5t0MbG9mM+OEuQz9gUFmNl/S4CLi7w30ArYkzF0YK2l74N8EWY3jzewUhVWSVjOz3vFe0i3f5ThOeVDCTUTF0tR9DDOAXSRdIWk7M5tdwP5hM5sfNZWeJRTWrwHHSxoK9DGzOYTCfaKZzYTF2ksZxiaF8YqIf7e4TSHUDDYkOAqAj8wsMz7vQ2AdSX+TtAfwv1wJJLWSPnplYoHbdRynuVEOWklN6hjM7D3CG/wM4I+SLgIWJfLVLvuSpaOwiYRJap8At0s6htq1l2ob/L1U/DGuP0al1X5mtp6Z3Zodl5l9B2xKWHf1l0BO2VszG2ZmA8xswFpbLTW3znEcp8lp6j6GVYF5ZjYa+DOwOTCL4CwADs66ZH9J7aKm0mDgNUlrAV+a2c3ArTGOlwiaRmvHdLpSHEvFDzwJnCCpY4xrNUm51lZdEaiIs6YvjPlwHKelYdXFbyVKU/cx9AGulFQNLAROBtoDt0r6P4JoXZJXgceANYFLzOxTSccCZ0taCMwFjjGzrxTWUn1AYTHsLwkLWRRiqfiBTyVtBLwUBh8xF/gJQUE1yWrAiJgewHlFPwXHccoGW1S6ncrF0qSOwcyeJLyRZ7N+DtuheeLIpbGEmf0D+EdtcZjZeELTT97447m/An/NcSqp0zQNryU4jlMGNHWNoUVz6feTU9l/0+GwVPYNPSyqnTX/N6M0HPHN1FT2/11+61T2D702PZX9YR3mpbJn+pdU/7igsF2CtNpHv+6U7p7Xb7VUq2ytXDYtnfr+9+2XS2Xftlu6/OSiHKYzuWNwnBZCWqfgLBtW1fwlMdwxOI7j1CclPAy1WNwxOI7j1COlPD+hWBptuGq5ieBJOkNSugZMx3HKHx+u2nTUpwhenvgLCeOdAYwGUvYAOo5TzpRDH0OdagzNWQQvz/3kEsa7IUpYvJkQ0DsdWBV4VtKzMWw3SS/FPN+XmRDnOE7LwiUxakTwNo3icU8UsO8L7A0MBC6KM5+PJIjg9SNISkxNiOAdbGabAocm4ugP7G9mRxYTv6TdqBHB6wf0jyJ4+dgAuM3MNjOzj4DzzWxAjHsHSX3N7FrgU2BHM9sxOsILgF3MbHNCLSbnGoRJraQxk98q8Lgcx2l2mBW/lSh1dQzNXQQvF0lhPIDDJL0er98E2DjHNVvH8BckTQWOBdbKFXlSK+mQ/rmichzHaVrq1MdgZu9Fuem9CCJ4T7EMInjxDX5vggjelcB/c9hmWFYRvJtqv5ul449aS2cBW5jZd5JGsvQ9EdN42syOKDINx3HKlFJeZ6FY6trHUDYieHlYnuAoZktaGdgzcW4O0CnuvwxsK2m9mMZykpaS9XAcp/yx6qqit1KlrqOSmrMI3peFIjOzaZKmAG8S1lt4IXF6GPAPSZ/FfobjgLsktY3nLwDeKyLPjuM4JYVKuWe83Pn44dGpHn7bnbMrYLXT0rSMGpp5kyaksq/YYudU9m9//Hkq+/Vmph+p3Wm9dP1anz/9UCr7K1vV1n23NO99WvD9bAkeOXJwKvuKNm0LGyXo0LOXUl2Qgzcu+VXR/+veF/61zuk1BM12HoPjOOlI6xScZaOUm4iKpUU6htgHMS7HqZ3N7JvGzo/jOOVDObTCtEjHEAv/fk2dD8dxypASlrooliZd2rMQkkZKOqSp85GNpC6STmnqfDiOU3pYVVXRW6lS0o6hrkiqrMO1tdWmugDuGBzHWZrq6uK3EqWkHIOkY6Ke0TRJt8fg7SW9KOnDTO0hWyNJ0nVxuChRq+kiSc8Dh0oaH2dmvyrpvYwWU570j4s6R48AT0nqKGlc1D+aIWn/aHo5sG7Ud7oyXluUHpPjOOVNY2klSeoq6emoDfe0pBXy2P0par29LelaxXH7tVEyjkHSJsD5wE5RH+lX8VQPYBCwD6FALoYfzGyQmd0dj1uZ2ZYERdTfFbh2IHCsme0E/AAcGPWPdgSuig/1XOBfZtbPzM5Oo8eU1Eq648lnirwdx3GaDY2nlXQuMM7MehEG05ybbSBpG2BbgtZbb2ALYIdCEZdS5/NOwJioc4SZfRsd20NR+vqtOPu4GO7JOn4gfk4Geha49umENpOAP8RCvhpYDciVh6QeE0BHgqOYmG1oZsMIk+NSz2NwHMdJsD9B4QFgFDAe+G2WjRFkfNoQyrPWwBeFIi4lxyBy6yMtyLKBJfWYYGn9omw9pUwcVRS+5+S1RwHdgf5mtlDSrBxpZfKVRo/JcZwypXrRwqJto8LDSYmgYfHlsRhWNrPPAMzss1xSP2b2Ulwa4DNCOXWdmb1dKOJScgzjgAclXW1m3xTQR/oI2DjKT7QDdgaeb4A8dSboOC2UtCM1iqlJnSQIekyXSLrDzOZKWg1YaGbppnU6jtP8SdFElGxByIWkfwKr5Dh1fjHxR/22jYDVY9DTkrY3s6VaM5KUjGMwszclXQZMkFRFTbNMLtv/SLoXmA68X5ttHbkDeETSJGAq8E5M/xtJL0h6A/hH7GdYJj0mx3GcfJjZLvnOSfpCUo9YW+hB7vLmQOBlM5sbr/kHcVmD2tJ1raQm5IcvP0v18Be2b9hF4douWlDYKMGCVul0aNJSavmpnJ1uUnxV526p7L9/Jddk/Px8vv5WqewBxr42I5X9ybvnHcSXm2npKu7tuvdIZb/vneNT2Q/aaN1U9pcO2bPO2kWTzzyi6P91/6vvWub04ojIb8zscknnAl3N7JwsmyHAzwiLqomwmNo1ZvZIbXGXzKgkx3EalrROwVk2rLq66K2OXA7sKul9gvr05bB4SeVbos0Y4F+ERdWmAdMKOQUooaakxkTS7sAVWcEzzezApsiP4zhlRCOJ6EVpn6UkfM1sEvDTuF8F/Dxt3C3SMZjZk4QOY8dxHCeLsm1Kyp4d3QDxu16S4zhLYVXVRW+lSrNyDAo0Wp5dL8lxnLSYVRe9lSoNVshK6hmHc2aOz5I0NGoXXRP1j96QtGU83z3qfbwu6SZJH0laMcbztqTrgdeBNSTdEGUl3kzqEknaQ9I7USfpoER4B0nDo5bRlITmUa58u16S4zjLTuNJYjQYTVVj6GBm2xDeuIfHsN8Bz0RdogcJ6zZn2AC4zcw2M7OPgPPNbABB/2MHSX0ltQNuBvYFtmPJSSHnx7i3IGgeXSmpQy35azC9pKRW0q23jS76gTmO0zwoB9ntpup8vgvAzCZKWl5SF4JQ3oEx/AlJ3yXsPzKzlxPHh8Wp5K0IInsbE5zcTDN7H0DSaGqmmu8G7CfprHjcjuB48k0NbzC9pORMx7TzGBzHaQaUcE2gWBrSMdSmZ5T95IwaHaRcLNYvkrQ2cBawhZl9J2lkIu5834iAg83s3SLyvUR6uF6S4zgpqIf5CU1OQzYlfQGsJKlb1DTaJ3FuCICkQcBsM5tN0Do6LIbvBuTUFgeWJxTcs6Pa6p4x/B1gbUmZqY5HJK55EjgtNgEhabMU95FGL+kESR1jGqvlErVyHKfMseritxKlwWoMsSC9GHgFmEnUGYp8J+lFQiF/Qgz7PXBXnMI9gaAGOIfQJJOMd5qkKcCbwIfACzH8h9i89JikrwmOpne87BLgGmB6dA6zWNJR1YbrJTmO06JodK0kSeOBs+LsvGR4W6DKzBZJGgjcYGb9GjVzjcx3U19O9fBbrbtJQ2UFgDY/zk9lr4qGHbuQtkre0Pn54cvPUtlr1Z6p7NNqJT3XaZ1U9gA3PZVOy+ixU9Ituf79R++nsm/bNV2l+o+vfJDK/vm3/5XKfvzQU+uslfTSsbsU/b8eOOqfdU6vISilmc9rAvfGeQo/EoSfHMepJ9I6BWfZKAdh0kZ3DGY2OE/4+0Catv864XpJjuM0BLZoUVNnoc6UUo2hUXG9JMdxnNy0WMfgOI7TIJTwaKNiKSmtJEkjJdXa2xUlK1ZNHN8iaeOGz91S+egnaa/GTtdxnNLGzIreSpWScgxFchyw2DGY2U/N7K2GSKiAiF4/wB2D4zhLUm3Fb6VKkV7tGML6ytOA2wmTvMbFsHHAmtFuJHAD8CxhjsEOBC2kt4GRifjmAlcRRPHGAd0T1x8S9/sT5jNMJvQF9AAOide+S5hT0B4YDwyI1xxBWKnoDeCKrPQui/l/GVi5lnsdCfwl3sNVBO2jFwlSFy8SdJvaAP8Gvor5GAJ0iPf6WrTdP0/8JwGT4nZSPpuUbx1u38zy5PbNy76lbcU8wE1iQbxiPO4KPEIQmYMwQe2huD8SuJsgD7E/8D+gD6FmMhnoF+0MOCruXwRcl7j+EKB1LIQzDmMIMDzuL3YEyWNCLeLfBPmKVsAzwAGJ9PaN+38CLqjlfkcCjwKV8Xh5oFXc3wW4P+4fl8l3PP4D8JO43wV4jyAWmP5LgUluX3/2pZgnt29e9i1tK6bzeSdgjJl9DWBm38YJaBlZ69tjYZvhETMzSTOAL8xsBoCkN4GehDfsauCeaD8aeCArzQ0Is5afjrOIKwkzoWtjC2C8mX0V07sD2B54iDAvIrNoz2TC+qi1cZ+FJfEgSGKMktSL4GBa57kmrVCf4zhOSVKMYxD5xekyJM8viJ/Vif3Mcb70suMX8KaZDSwif8lr8rHQ4msCUFVLPjIkRfQuAZ41swMl9STUUPKln0aoz3EcpyQppvN5HEHmuhuApK6EZp7D4/mjCLpEadPNjD46Msf17wLdY80ESa0lZfQgssXrMrxCWJthRUmVhP6GCSnzlYvOwCdx/7hEeC4RvWUV6stmmNvXq31jpOH25W3fsiiyPe5YQofuNEIbfE9CG36uzudM53FP4I1EHMlzcwlv4pNjPLk6n/sR1jKYRhDM+1kMP5j8nc9HUtP5/KdE2nMT+4eQ6AjPca+L8xCPBxL6C16IeZ4Vw7sSOpoznc/tgZsS6T/a1O2Evvnmm2/LsjW6iB6ApLlm1rGwpeM4jtPYNMd5DI7jOE4D0iQ1hlJA0vnAoVnB95nZZU2Rn0JIkrXUL6sIWtrzkVRpNSPnHKdeabGOoTkgaXXgZDM7Px4XLPziqnGtzeyT2uzyXFthlk7opT4L5GWJS1IrC2t4tDGzH1Ne297M0i1CkZL6dliSdgA+MLNPluX7KjKNJfLc0pyu401JTU5cfyLvaWBnSVcCmJllRj3liUuElfD+JGnNlPnYATiqQH6S9r2XtcDIdQ/JuCQtL6lNgThWlbR6dAp7ADdL+m1cFraYPOwKXCOpXW3PdFmRNFDSasv4fGr7DvYA3pK0qplVF/q+8j3r2uwT38OaEH53xeW8YWmI78nJjTuGRkbS0XE7ESDfG1/8g/4H+A2we1wmNa9zyPyhzexkwoTAXyXFBgvkaZ2YzqvFvIFKagecTpzkWOgPK2lnScfEpVeXKmiyCqPfECYjXiMp59oYcTjy0cAwSfsA5xGGJq8C7J9Jp5b89AJOBv5mZj/UVvAlhh8vL2m5QnZxvzOhmXJAPM77P0vEv5Gk7SD3byJjZ2bnAVcCk6LjqdU5xN/LTpJOjt9Dq9peMBLfw2nAuUqsW16bk5G0gaTuklbLl5dlJZFurS8LWfkZKOlAhXXlnZS4Y2hEJJ1BkBCZB5wt6dh8tvHPuy9wBvAScKCkaxLnlG0f09iaUNM4ErhJ0hoF8tQLuBX4yoqfnFdFkB/ZJJl2nvh3B64GPgX+JunMXPcabbcAtgYuJNzzLyUdlsO+ijBz/nmCU3jIzIYDlwL/BDaT1D1PflYETgPWo/DEzcyz3g94ilAr+Wk+uxh/X8Icl1eIc31qK7xj/HsAjwFXSBqnMFcoX/xbmtmlwI3AK/mcQ6KA7EfQLxtAmNtzgaTWtTmH+Lv8CXCxmX0pqVMir0v97qJzHgX8CvirauYc5Yo7k69Bks6UtGMhZxLT2A34s6TTJW1VwHZ3YARBmmaiCig2OzloyLGwvtVsBM2l2+P+eQS9qUpguTz2HYHngMHxeD1CoXdpLWlsTJhDsRFBM+pe4BagR5adso7PJUxa3Dz7XJbdJkDvuL8aYQ7HHnlsK4C2BMmTTQg6U68Aq+Wx3wX4D/CreLwccCChQD46V95jHm4iCDb2jGGZuS39arnfvoSC7P+AtQp8b+sBDxOUdHcF3gJ+mcd2IPAB8Pd47/cBfy8Q//rRbpN4fDtwPzXaZMn77QxcC+wTj68APgZWzTzzrLh3jr+BLePxjsDfgKGEfqhc+WlHcOQnA2sD5wCPAzfnsV+XMJ9nJeD8+OxXyM5L1jV7EKRiTiLMSTq9gP3g+Ny3IcydugZok8NOhP/ZA4T/wDaEeVArJ20a8n9eLluTZ6AlbPHP1jYWMCPiD7ddPHc8sFWOa9rHP2TfeFxJmHn9OXBlnnQ2IDicFeJxa4KC7ROJgjMz4GDn+Ic8MsZ9HjAmFppL/XkIb/LXAP+Kf+hBhIl9R8fz2YVSm/h5KUHZdgKwQQz7CbBXjjSGEZxHu8RzOxwYS5hlrsR9DoiFwMoEIcaHCYKNaxEmRG6Sdb97EFRzrydMvhwQ7+dsYO08z7NXLOj+ngjbMsZ/ZpZt65jf0YTCeihh3fIXge1yxJ0pxK4kTIrcPXFuZPzOuifC+hKc7RDC6LlM+B8INdAeOdLYiyBFc1biee5AeFm4LPl84v4vgN8RnPSbMQ9nxGvuJDTVVWalsQrBQe1LqOWtF8MHAV2S95vIw40Eh7IV4eWiRzzXNs/3cG7MU3+CKvEaMXyFGN8q8Xi1eHwm8EeCkvK6if9Z36YuC5rL1uQZKPeNMGv8yLj/a+BLoE88PiYWCmsk/jjrAyvGguYUwhtSZmb4LsDFwDbxOPlna0eoZYwmCB92iud+Ev9MvRJ52ofwlnc4oSC+IIZfEguDTbPi35DQbNONUDCeSnir/Yjw5rdK1j1vCFwe83QSsAjYOJ7bLN7zoHi8d3wOW8XjG4Gnicq0BIfaMSvv7xEcwURCP0cf4DqCxtUYYLes/GwfC6DDYr7ejnnsQ3BG51HjjLJrFxfFdPpQo7i7DcFBrkUo4HeKz26H+IzOIRSoJxIK5hsS1yrrcxWCw7qEJVWD7wT6x/1cNZGbsvLYKyvOzPe/I8Fx7J94njsSHWcijkMJTnP1eLwy0D7uH0BQKVgpflcrEd7iryK0+08kNBWuHO13ItRuVyfI0a+c+F20IvRnPUD47a2ZSGPbaJ/5LfQjOJADCbXnycQaJ0HB4FSCgz8nxnk/QWX5CmAW8TcPbEqoSS/loH3LU241dQbKeSMU7K9T89bSGfhl/NH+PZ7bJGG/V/zx/46wHsSKhDfuDwjt7jOJTiFxzf4EZ/AgocA6Jf7pziM0B4yLBUHmraqCUPiuQihUX8r8OeP5SwhvZplCJvNWNyQr3bbAz2PaZxEKyEwhOZJQ9b8gpncJ8Cpwc7y//WIcvya81Q2L8Vwdw2+K4csRnObNMbxrvJ/N4/EJ0Xaz+GwvjAXF6iwpa3IWcEni+FRCE0YHQs1poxieuedtCG+Yu8d7Oic+3z6E2pWAzon41gR+SliP4864v1fi+W2QFf9ehCaju+N3sxahMPs9sHXWc66tJrJjwi4T936EBVR1LAAAIABJREFUQvlpwkvEGgTH+A01cjNK2CumcTdhfZGOMbwyhh9PqD1kmhAPi3G9Q00Bvg9wG6H2cxDhZSbjiDYlFNhnEl46ehFqPS8QRCeJ39k7BMe6YuLZvECoHfYn1BpPJTiizQkvF3sSHNjdwNfAKYnf5iMxT6MI66Ps19TlQXPamjwD5boRCqqXCDWANvHPcCFBTrw3oWlgjYR9L8Kb/VqEt82XiVXx+Mc7ENghK41B0W5lQsH1eAzfm/AGNYrgOG6If8zM29atcXuWmqr/AYS3wApq3tZ7xT/ZJ8DYRLptEvv7UFOgb0Fo79+W0BF5DaEDE8Lb396ZeyAUyg9T05bek+AsT4jHw6l5m9yU2BdAcDoHJdK/Ergz7mfecAfG59stHv8EuD7uVyTiWSvH97YXwRFeA9xBkG1vRXAO/yDRHEEooC+Pn2sQamx3EGok/yM6hKz4B8b49yYUurMIzVwrE/oPLiU0MWWc7MWEgj1fTaR1Iu61CYV4n3jNaQSn2zb+DuYT3vYrEtdkmh2XJxTgDyfOdQC2Y8naSCVBMPLfie+kC6FN/waCc9szhmeuuY5Qa/l5PF6J0AQ2mtDpPp1EwU0YcTabuAZLDDs+ficvEGoPGccjwm99BKEGMyhxzR4Ex9s/mR/fiii/mjoD5bjFP+QAQhX/FUIBfTuhyeDP0aYy6w+6KuEtf694TabAHkzNW9yqJFaeIjRTHUIo1F8E1onhy8XPzAJDOxAK2pPi8W6E9S0yiyVtR2ie2YbQlzCaUJi+Q2jb70roGP5rIu1M3McQagedYz5uieEivI2PJ9SA1icUep0Ib8BtCB2DmdpDK0Kz018SaSTbv58mtu0T3sgziz4NIjiUyuR1hEJ6bIyzYyxQzgXWiff5DuFtdMXMs47XjgD2jvutgb9m7jsWPJnayomE5rVNCaOQTk/EcVD8vteN39kpiXNHA9ckjnclNMOsFePaIHFu7XivtxMcTnZNZDtCrSvTn9MHeDpx/XqEGkyms3qJlQsJb+CjCbWVXQjOZzRwT5ZdpoBfIRF2YvzNZDq2FzeH5vg/HBef3SRgYCJ8XcILw0YsWYvZhNC0NZEla3o94u+nR8JuVYJTa0eood5I+K2tRp5VFH0rogxr6gyU20Z4E3w0/tFXjwVBpt32xPgnb0eo8vcgFKYXx8LrTeALat7Ytye8na0R/zg7EArI0+L5A+L5idQ4hSGxEGlPTeG9JeFt91PC29VahP6FfxHalacTC8Nofy9hcaMDE2ErEJpfbs6634GEJpf1CW+9rwG7Js4PI7xJHkF4A+1P6PSuJDi1f1Az8uqnhIKsXa5CBriL8CZ5NcEZ3EAo4Be/PSZs2xI6RB8m1LZWJDQ5jCI43r1jOqcTCuBMbeNu4Bdxv5LQbHFrdn4Ib/cbxu/3KYJjSzqnTGG9SdwyhdmuhKG2bajpd7gW2Cxx7dGEwjTjNDvEa5aoiRB+M30JheNKMQ+PEUd2RZs/AOfE/czvQfH7nxivexv4QzzXmVBLui3rfveN4TcQ2+oJTYnvEppHP6HGaWYK+EzNuG08Pp3QrLMuofnttCz77WO+to/HfQnO9/8ITvwGal569iL8X4YSFJpXj3k/nzBo4zPi78q3ZSjHmjoD5bQR3tBeI0ycIqug+CmhTyHTVns8oZ30HeKQT0LtYDzhzedwcqwdTXiTeopQ0LYhtKVeTXgT3jHGuVfCfgtCx1vv+Ee+J362IRTmfQlvmsk3tv0Jb87PsWRzV5dYEPSmpklmX8Kb/8OENt1zCc7maELb/0TCEMlrCA7pIILj+SmhieIogsO6AXif2Emddc/JmtUdhOaXrbIKkUzeBxDeQjOjkvYkFJaHxuNW1DSBiPAGujKhCWR1Qm3iQ2qWgt0hPoeVCM1sP4n3dRqhLf+JRN7OAw7LkeflCDW2TOE7hvBm2z9+Z+8RHQOh8HyR0AwyFzgj67u/PT631ol7uIvQf7MKweH9PX5/u8ZnmmxeyfQFnU2o0R1D+D1lHFlXgsNZNXHNToTf4loER/4CNbXNgwg1wV2zvrO9CL/tv8bPTEfwafx/e+cdblV1pvHfoqhUURRUioCKgugNohRBsaIIEWPBENSLiIKixhgYSxRQiQXsiUmMxp7EaDSWADIqOsZRiQiKsWQmjk4mbVomTspM4pg1f7zfcq+z77lyzrll3yvre5713Ht2Xbt95f2aFJK3iEKdo+1PQZDXHFu+h53vdTLLZ0/0Tu+CLMJ3kZAYbOvHElkmadTAy4qewCdlAH3s73zEPGPMdAiyCkaQMbAuSNtfa+s7GePZE+HfS8kERuy0fABZJC8gptrTmMK9SFOaktvnBKxPtf2eYh/gxTTMbxhn24druRo5jbshoXUypVr5WLIY9pnIAXir/f8CElrzEcb8NmKwjsz5Ogdp7UMRkxzwMfc3ZrSPU8qQO0TXFuCmDcBhtnyy7XOm/d6KDKrrZ+tDfH9/xDB/afP+KRl8M9nu8bbI2nkRONTWHY98B3sga+UoZHEMR1bceATTnG/P+QYUNvp30fEH2TvR07ZbgyykS6Nr7UZmbX7GrrWXvRdXIeUkaNfXYsy0zP08wea7Jlq2AAm3WEnoanMZRQZzhryXeruX+VDl4fYcBiEf1PtIKQrReCMp9dVsZ8fdAwmznyLhsdDWb0kmzEfYXHZDQnU9smZvtX0aKBZp1MDPip5Aex/2kQ9DzsAQRlqPtNSp0XZbRB/bzvYxb4cgg1VYNApRtEvuPNshZldnjOlIhKHHjtjeZBphYJa7IithXLTsNuRo7B/tOxFpcU+icMgjbPkyY1BvxOeydf2RRTIJCYhdkPa5EgmNLkij/BOKZInhqpAtex5RvP6m7nX0/yOUNmPaHWHYQ8gaS71LpvlPtuuaYIzlbJRf8TJifqPsWi+3ez0AMfRZtv9Y2/+y6JwXIUtgNbL0AuPrYXNYizT24EQfjbT78+23IwtFnomExzbICnvelp+A3q350XljazM4envZM72GzOneAVlD4b07zq57tF3PvUiD3wNZX+uRYhK2PxxZel2i4wdI7Bnbf6fcMxqJhNfuSJl4GSk930Aw6Yjc9ofZ+fvYM3glWv5XzGFty/ZHkU0BSvsSmd9sLlKYDiyaJ3wSRuET+KQMZNL+OxmDPxnho0EbDB/bZMTALkdaXgdjMI8h7e+3WB5B7vg7IC0tfPS9EH78BhlWGxj/JDtm0JAvR9rkXOSwfAJzGtr6utwH90X7kINwGA4Mja8jN7cvk2Usn4w5z+3aQoz5XfYh7xftdyzS9HpVco9z1zgbCZZOSKPsj7K9D0TQVncEl7xPhtXfihL6uiC/wR+AL0XHHoOspGsQgzsTCcX9kIZ+E7I89o/26YOsjt65eR6GhOEaShP29kVQ0rJwPUgTX0EmJI6N1h+PNP/dadzaDP6RXog53oIUkQHIEjoGCaqNdn0vIeFyBgoMWI0E7V7R/MdR6k/YEoUKL0DCYyUW7ZPb5xWynJUvksFnU5D1GedqjEIWUwh7PZwswmwUCrsO+S4jkJU1I7pvl9ocT0PfRknYcRpN4GdFT6A9D6ThTSRzIp6KtOMQqTGDKCQSYfkbENO8wD7QLtG2l9IQPuoZ7X8T0jh72u8TULjmqGibMUhLPQflP1xjy+uRD2A10tZjSOgkxECD9uWQkLqbCiI7kLb5jDGCZzF8Fzml6xDj7Ip8D1chZjbdGEm3Gu/9QUhgfQYx65uR0JsMXGDbTEcC4BD7fQ2wh/1/CNKGl9oxwv2eiCyHXVFU0oWICe6OtPmrkEAul62ed1B/yp7zg2S5LAPtPam338ESWRLtN8We8z0o4msglVmbPRAMVYeEQhDM1yEmG4T7wZRGX3WLnwMZ1PVHSn1MB9j7sx6zxKJ1Q5Bw+Wy0bJpdxxUIRoyFwkDk6L8+euf2QQrEnUjhGR+tm4ugu+vIwrh3QErJ7USBEmk0A28regLtcdiH0xlh0F9DJm5wBt6ATOC97cU9g8wpNgwloB2OtKcQSTTGjhknHTnEwO9HAmFfxKyWIuEyDzktx0bzChnA9fa7GxIOy6Ntto3OcWjEVE43JnNMtO1CcqZ/I/ejJxI8D5FZSF9AWtw1xhzGGuP6in38v8E0vBruf7hPvYwZzbD5v4qY7FeRZvsTspIiDmm9Y+1+B+f0F+3+HmLLTyDT3OcZk1pr96YOWQhLbZ99ysxpCrLQLrdn2sOY1/dtnvch/045S2RMdLwDkMCKLYXGrM1H7X6/j96x3ig0ugOyLubaO7OY0jDj1Zg1U+Yeb4XevafIHNPhHg6Mr9n+PwRZPSvJciP6ITjuFiIY0dbtigTGeqL8HPQtnYL5bnL71Nu7czSZcOhAZkUmS6GZRuETaI+DzDnbyT62m8hM3qMRpj/UGM0jCNftjwTFP6OcgGApTETaXL6sxKEIEtkVmdurEdPaCjlt59Ow9MMJSDO7iaywWncU9XOn/Y4/5jlIiAVTvh5BWifUeF86RXNfbf+HpKSP/Bx2TWWL6VVxrtFIyMaO2eMRlLESCYoAIQXGOtXu6YXIF3KMMbvzkGXxn2SCbTTyuWyDYKV5xvh2R8x8ETnfCPINrLd9NyDopQOCdS5CTH0qFVoilEa1VWNtdkYCYV8sqgkpKDcDp9jvY+29K1ufyLbZCsFvj9OIAMltPxYpRlcQ5TxE6/MWVT8y2LKBbyB6brFv6SxkIZxAZE2n0byj8Am0t2Ev5nNIM11uyy5FTsjvIUsgDvWbgJx05xsDONoYZT3SCF8lF4ePYJdLkGk92Y55OYJrjiNX1wc573a0/cagXIAZZDVqulOqlW1NBn/NRppmgH/mICHUFxqveJm7J7Gl0xHlBeyMsN8QY363XfeUSo65ifOMRdbACuTcPJjMYpuB4JfeuX2GIqHZHwmNDXY/A2bdF0FTYfvxwA+jcw9EQv4FpJXni8l1QZbKCAShrCGDXYJGG6y1TVoi1GBt5sYByNe0AjjVtj0T+b1WERVo3MQ972LP7m9pRCOnVNk4GEFxy6nAd4QCFs5DsFmjjmNKhcO5tn3fTR0/jRq/taIn0J4GYtKv2Uc6yJjEN21dqFO0R/4DQhrnfahMxK720f4AaXB55/QRSAh0RdFHj0VM7sdIAO2Ym9NGpMH+FAmBE43xzCKyRIxh7G7M50BKhcMfyCyHirX5HFOot2sM1xIX/FuMNOKdKj12I+cbgxhuyAe5AsFTE8mEQ794bsiy62HPbQISxoMQ/PKOPbeS7F7b51ng4ujcixA0NiA3p0l23duSlTYJQuC/gbX2f0cqtERomrX5OQQv9UMQzw+A02zdOciZvUMV97wLm4AUc+/BYchyGFrh8XdD0F+Dc1AqEOL/dy6SF3zSR+ETaC8DOdfGEaXo2/LnKF9W+Qhkhi9EURxDyIRDj9y2sSZ8C1kkSCjlMMsYxwpKce1BSFjsYszgJ2TY63SEEe+U+2h7IPz5RiTMAn78JLIcetR4f0LBwLikw9UoZHQBEqgDazl27jyTULXWEPLZGSWn3UH5onKHG5MKQuNkLPQTWRffIUtKOxvBdl+2+70fsgy/bQx1HQ3DM4cjDTwIqgE2l36IuV+BhHDVlghVWJvRPuORryeUDOmOoL3vk0WvbRPNZ4TNry85Kyg65kcYPg1zFhrbp3d0jq0o0z8ht33I9j/E3qVDyaDJEuEQHbfsudNo+ih8Au1hIBP8cfsg36a08cetZCUdwgs7DEEJX0QhmhuRUNmNrNJkl9w5OpLh3HEd+6OQ5voaUSQI0i4HI41zOhIQIWlrkv3tE81pHII4DrbflyHhMAlZMNcQhZJWcE9i+KgXYrAh3DUusjcfS+5rxucxDUWthHLmnZA1ki8lPQlp54dFy2bbvZyPore+hkJ15yHr7CBkbS01hhkq3C4hKr1txxqMrK+nomU7I8FwG/LthIioiiwRqrQ2iZLRbPs5KHrnSjLIsQuyLO+jtCrsNCTMv46EWzmHb7Aqe6L3+ggk6M/IbxO9Dx/5Bsiq/z5i+5YLdw7n2BdZvd9CfoQlZAK9Q27bUP6ipqi2NDbxjRU9gbY+kJa2kSwS4zKkBR9jH+sGDO+19aNRSYVTo2XHIIy2izGbutw5AkPvisWUR+tCQ5ch0QcX+ggsRFDIz8m0q7HGeOI5HYkSopYhzTQUuluAmNg7lJbRqDq6A/lXzswxif2pwGlZ43MJJcpnNbK+ozHH4ICOq5DORpFBRyEBeybC+0+39f0Roy4RlmS9DHohK/A423cFsupCTaAhti6Ula7KEqFCa5NSJjzf5tLB5nIjCkMOFmEXom6ByKJ5HFkU05GA+EjLD/fQ/m5tc6xHwvQie0/vKbNt+NsLCdofI2vk++gb2DL/nKLv7FdkIbQNus2RveNbI3/R+KL5wyd1FD6Btj6QJnmx/d8xWnYZ0izzWqpDkM7aaFk3pFHHmcbhg65D5RdCYk9XBAHd38h8piIN90VjFstQ858FxoBK6ivZfO6nNEP6BbLEoy2oEeJB2bph3vWI8YXaRSciLbRPCz6bo5GjeSfKwApIm/9Gbtk+iNGGGPlJCL5YhhSAkJy1A3ImX06G6fdG8FMoOx2KI85DjHgmEh5bIsFTjSXSFGvzHHsfQoOlLVCI6vXIgijXBrM3ygm4jNLKvBOJWnMiJvw0CiS4EctTQJDkauDu6JgDkMO8l+3zZUqr/w6y7QLcGTP6V/mYbnOUCpynSU13WnQUPoG2PpAJvopS7PxYsoJe4YPemyiqAsEVD9sHOBJZEXkhMhk5CRcip+Jdtjz0Kng4t31fFFUSIJuz7cO+AsEBCxCjC5EpB6GwvpspLVg2DFkKnaq8F3mYowdyiMbRWY+h7NtXiTJpW/D5bB/PDflb9kPMeYLdmxB5NAqFeU5C2vJKxLy3QYz6QgTTBOHQh5xgQ47VD+zZhpDgLijS6etIQDqqsESiY9dibXZGfojA2AN8tAWyIpZTmiTZk6yM+5X2nELQwUFI0A6Ptl2H/FeXIIUnFpQ97H38Lln132cQ3HoA5av/TkcKTTjG7rZ+ApvoNoeExUPk+pKk0QLfVdETaOvDPo6l9hFNRVrhy5S2yjwcYdnrkKMzaJKvIGtgCZGGQ5Yo9DhZlcrQn/nb9rsbUaaoLdsGaV4Ton1uR/jtcbltpyLrYQxZ17gQrXOYfcAVx4FTCsUMJauZ0wMl0V0fzXEsVUS9NOOzmmaM7lmUCHUuCoX8LkrUegerdYTgkN9hlpMtG4CEwxqi5DsyoTMGQS8H2D39WnhGtu8iMiFUsSWSey8+1tqkFOrZEvksXiSqKWTrQgnsIWT1oo5EjPpFxLRno2CHO+wdfYtI4UEKRCjPHYr3rUH5GgE2606W6d/Z3s+XqaD6r83hXbKkvbLd5qJr6k0EkabRgt9S0RNoDwNpf/OQhvldZB3EHa0uQXBAiGVfHn3IT1Pa/Sx+0ZdT2oJyBAobvbrc9vb7fKwTnP0+DOHVd5FpYd2xVp/RfouN6VxnDKDifAIkCJbZfeiDBNEplOZJ/Bb4Vis/l62i/7czphcY8KnImppgz2UkEhw7I+hiuDHKF4ELo+PUIfglPL8AqUxF+PpE+z0cCYJbEJRzO3KeVmyJUKW1mXt35pN1RPs0cnYHf8pMJCC3RxDQ7ej9fQqV6ZiBfFQnI2htOrJu4hIUHZCP6BFKHc0zkAVzPDn/EfKrBItxU9V/Q7e5sVTQbY4a/F5pNOHbKnoC7WkgDSiOuAm1YNaSaUH9kdPsK2TJSe9hbQrtAx9uTONYZIoH5/PeCPZ5m5wFEJ2zP7JgViHs9Q0EkTxOhjF3Q865w+13YED1SIBV3OrQmMQEsrpCnZHmeI8xiWA5LLHr7FvJcZvhWfREmmmI+tnWmFEI9d3CGM11uXvwGRSWG/ozjEUO0nORoLje7t92ZIJ2ENJ0h0W/ByMYZJ49g9DCtGJLxNZVZW3aurPsnRtkv/shJvw6si5+gqyVo+1azkGKQ6ygHIKEx8gy99YhS2QAKrtyb279KViTn2jZx1b/tTleYs/FoXe94m5zabTuKHwC7XUg5h7KVFyF4ILAcAegNP8R0faDUWz2z5H29DPkUPwCsipCdNDuqORBoxo9YopH2nZ7GXPbSGkY7TnGVAIzG4eEScUfHA3DIB9EWnJnFNFzDyrSdokxnlb9mJGP5S0yLX4hsqYCJj0FOaBDZM7Wdk9uN4Z3ki2vQ/kBa+25hh4Eg8n8BY8iReAGe16/jM47kSosEZpgbSLobiUS1r0QHLTE5tYbwT87IH/FSDLhdhqKjPocmSP3q2RQZvyshyMLs7fN9434WmybfC+Pj63+i6zKOmroNlf0t745jsIn0B4Hgla+hzWYt2XnIg02OPICBhtC7YYj2CFkAs8jq5wZ+hAPQZrcRirPGj0YOVTzTsl+yPH6HBJc71BjOQqE069GDHUlslS2RFj7BQg6aLY8hQrmEzOx0xDEM9aY4pUoWW8Jwq9Dv4IA48xB4ZyHIYjmZFveldICg9saE1tsyxYjp2kIQb0AQVUBdqnIEonmXYu1GQTKJXaNjyELczlwVZn71NXe05DYdpa9g1fas/sZZTqdIaFyF4qK6oUspNcorQBbafXfa8m+iaq6zaVR3Ch8Au1l5JhRd8Rsf4gYesBBFyIzviel8fxDEYzwkjGJoLEtQnVowv6DEbS0yRo20bF3pJHyAGSd106kTJnoTV0rWfLak2QJWhORpnkplixF5Jgu6HmcjhKjRtmzOQpZEwEzH4eip+Yii+wlFBE2AsE+n4uOFWfZjjcGuYBS5/sYe84hWbAiSyTavyJr055tB3svTkRWzP4IhhlP5uOpt+N91CvbGO1FKCfiIbKSGLPtfHdjMFw0rz3JLIx9EdNehKyZXewe7xKdo5LqvwtoYre5NFp/FD6B9jCiDyFkDwdc+zKksY2NthmU2zfUv7keQTkXkyXLTcO0wXAeCq4YSSnD7WfMZiNRvXtjOK8iLbojBZn7NBQOb5JrAI8w7f4I5liPcPfrEcSxPQpt3TW3z0iyng7jjEEuQUJnN2QZBiy8YkvEfldkbSIB8VXkGK6P7vdaZMGFfguzkW9hz+h4w+xdCzDiocj5Pdt+z8f6UkT7dDUGfS+ZcBiNILGvIJ9Y7OyvqPovVXSbyz/TNIobhU+gvQwazx5ejJyc46JtS7JH7eO42PZ9FmlZi5GDeJONcAq63rMxDBhF2NxNVtb5RKThtVjyWm4uO5HLAWnkXp+JoJdtkZDdHwnvPZHW/Zgx79NRMtXi/HGQJn4z0o6/YMv2R76FpcaMg2CvyBKhBmsTaennIyH2EFnb0DH2LGbasRZQGlq7E4Jq4jIdXRFEuRrLqcjPy37vgkJwbyWrXbQYRSbFmfebrP4bbVtRt7mi3/c0ct9V0RNoD4PGs4eXIo30ehpGm+xP5tjrSJYktARpc3cQNdlpSwPFlz+HtMQ1SEDMRRr5PZiTvBXncylyxO5FmVLgOcYbdxwbgLTXdUg4zyWLkjkN66oWbX+gXdunkQ/hfqz1J4LQvkHWia0iS4QarM3od0dkLaxDwiQkrx1j96MjcuKG7UN022xjurPIfF1dkTUzCkFfAYaajHwOF9rvgUg4rEBQ0bO5OVZc/ZcKu80V/b6nUeabK3oCbXVEH8JBNJ49fKf9X64cwxQEKYQSAp3tA1yFTOjHkdOwd0tdQxOuvR6F0s7CGu7Y8nEIQmhyldRqnoH9fwty/pbNpsZ6QTSyrs7u+/PA2x9zvpMiBtkdRf48T9bPOmQMV2yJ2PYVW5vRPlPJkvFORrDSHPt9jDHurXLneBNBXZ2REAxlOkJUVgfkd/o6sq5CkcHTkPB5wLbrgiykB4hwf6qo/kuF3eaKftfTaORbKHoCbXlQWfbw1jTS0Iasf0OAFA5E+PL2SNu6n1wXsLYwkHb8DvCjaNn5KBqp1c1+JJzvN+b6KmWc85TW0pkODRrp9EF5Fy+R08yjbabYdYeIoI7I+fwQcGK0XcWWCBVam2Xmsi8SZiF881SE5T+FrIG9o22HIwx/QrRsCySovonyDmIhewRylN8IzIuW/xj4XvS7e3QNFVf/pYpuc2m0zVH4BNrqoJmyh5Em929I4/sHSnsGVFWrqJWv/XrkRzjIGMsrtGJIajSXIcZ0Qu/m68iFx1JaBfQFPr4TWAgfDhbhIUj7Pxlh7OcirXYPZGk8igTiwjLHKmuJ0ARrkyxktiNygj9K5gifQ5kmOwi2+iiklcxCCP6tPXP3qQ5BPt9H/oido2O9gfWLoCFE1Vj1312RD62m3uZFv+9plPlOip5AWx00b/bwCFR1ckKl+xQ9UDTVXAQP3EMrFMRrZB69jTnGRQx/gARzHaVC4WmqiINHFsJ6BCE9iaC9vih+/2Wk6Q5DQuNuBNHkNfsGlogtr9raRDkp75E12elk+79IJhx6Re/iEITTD0TW1OToWEdhDYly8w09GEYjoXgv8knElX/HR+fYZPVfmtBtLo22OQqfQFseNEP2cHsfxgxbLU8hYkg9yXInbkfO+6BNH46w7RAq2g1ZNBWXYkZw0GJjZIcjWChumdrDjnswsvTKRkXF9yn6vyJrk1J45ywkoK42Jh9DRXciAR2HvU6l1JKabffpPGSpbCBn0TYyr6kILptHw1Driqv/UkO3uTTa7ih8Am150IzZw2lUdd+nGSNchWCsaQg+WoZyQl4gSthDkMkmLZpI6IxFFsGdyMp4niwE9Sg7XwdkrXyJKp2kVGltIstsHZlFcbHN7wBj2N8m8kUhbf91skKKPZHWfqDdp7ignvuYeYVQ2fNtn496ZVNb9d+Ke5un0bZH4RNo64Mas4fTqPl+jzFNszfyc/zYlu9jjPXawNhsebU9JUYbwxuN4LJ1ZPkKBxBVULVlNTnbqdDaRBFAoe1lHxMSSxDWfxNSSvbK7bMfsjxOQnkMa014jEG6eLpLAAAH+klEQVRCMsBr5dpolpvXdzBhldu2kuq/Vfc2T6Ptj6CxJErUJsg5NxnFvX+AmMtM7/0/Oed29N7/OtrO+RpeXufcJMSgL/TeL3fOfRY5nH+NspIv8t7/0DnXwXv/1yZcRz+k7U9Egm46cK73fkWZbc+wbf8FwVb/jCCuK4EPvPf/Y9ttDXikrJyBILCvIr/EkcCr3vtHqpzXZ5EPZJX33jvn9rRz/BrBaOej9qk/c87tjRzgk5Al9SYKX12BktpORILtPxAk9jzqoPc/Vdy6RG2BipZMaaThvQcrGogcn88gyCPAO0ejaJhtaSRXocpzTUMMOHQK64W04ZC81iyaLRVam4ip7kfmQ5lp92AgWRjskbbsZcTM9yCzDPZG/oaJTZkX1VX//QJVdptLo/2MZDEkKoyC1u+cG4pgk5e994ucczcgpngtgle+jKJyGmjbTTj3FJR8dbP3/u7mOm5TyDnXAYWXnocinUajZLoNSJgtQPdlGoKabkOZzNcBS733jzbh3MOR5fBt7/0Lzrl5ZJVhO6FIo9+gSqs3okY9DwN/9N6PsWN0szn9jff+F7XOJVHxlARDokLJOTcNwSL/hSKFnvbeX+6cW4R8ANuiaJhVtcJHmzj3VUhT/k1zHrvG+XRFWvxL3vu3jNHORMJhB+/9kbbdWJS4djqCnXp5799uArw2FJX76Ir8D0967z+0Z7ALElYeCYWHkDC9y/Z9DVkSpyOh9RBKdHujppuQqE1QEgyJCiPnXHeET5/nvd/gnBuDnKOve++vsW16eO9/34Jz2N57/+8tdfxqKWbuwa/inPscYrx3ofyBPzvnrkX3qUnWjnNuRzt2L5Sz8SPgPu/9z01wTvPez7Ztg0/j97bddd77XzjnXkEWxW1IsP+oKXNKVDx1KnoCiTZPcs51Bv6CwiK3tMWvoyihk5xznb33S1tSKAC0JaEAEAmFHsBtzrl13vslzrmtUNRRnXNuBYKTaoKOcpbFvyGn9/+h+z8J2Ns59xZKxLvW9ulo5z8e+R+WA593zt3kvR/lnHsa1UlaUuYcidoZdSh6Aok2P3LODUO1h/6CcgkWOedGeO//hCJdngN2NYhjsyHnnIt+/gnBOnXOuQu993egKJ+DUSb2GbVq5ubX2d85N9N7/yHKbH8P+G9U92hrVBl1off+UbMc7kNVZwd4RRldjZzmFzjnBnvvD0UC5Y5wjlrmlqhtULIYErUKRY7miSiBq8744JPoPfyhc+5OVKlzBoKUehc03UIoMGzgD977jc65DUg4XOGcO9d7f7P5IZ733r/dxNNtY8f90Ht/v3PuQVQzaiRqzDQcOM6E1Vkoq3of4Grn3L96719xzi1DYavdbP6DnHODmzivRG2AkmBI1CpkTG8c6gY2C2mfU1CZ6m+i8NHtUImG7ogx/bKQybYyRUJzMLo3k5xz07z3rznn3kTZzH9j8Np1zXFO7/0K59xfEaPv4L3/jnPuCeRsvhb4EIWt7ga8471/EHjQOfdr4BvOubO992udc583n0dn7/0H3vt3m2N+iYqlJBgStSbVoUzm9cB659wsVIK5Eyr3/GdzQF+Fkqp+XtxUW49MKByNrIPJCOv/rnNuuvf+J865f0L+hOeb+byrnHMeuMcslUnA3OB3Maf3hcBetv4ls1q2BL5ly/5ox/qgOeeWqFhKUUmJWozyDkhj+vOBG0044Jz7HnJ8Lvfev+qcGwj8xXv/m0ImXQA55z6FIo5meO/fsmX3ovDP51Dpi5O893/fQucfgUJi3/TeP2+WXR/g9977Nc65yxD09B1grQmyQd7791piPomKpyQYErUI5XwKewJ/RiUYzkJRMO+gGPwlwK9Qtmz95ui0NGf8Bai8dl+UlfwrJDBXAv/hvV/TSnM5EkUcrUIVU9/03s9xzi1GRSXv9N6/2BpzSVQcJSgpUbOTc66T9/7/nHMHo6ijZaiGzjao4NuOKHGrL7Ig+qLSzB0RM9zc6F9QmO4pKIv5YVRS4r+89w+01iTM0TwLtSZ92Ja94JxbirLErwZ+11rzSVQcJYshUbOROU9/671/3+LurwE2eu+/5Zzri/Dq33nvLzMm1BOFXy5CNXdeK2zybYCcc1t47//inNsXQUuf994/3cLnDJbdQajl7AHASu/9E7Z+GCpxcapzrqOFtyb6hFPKY0jUnLQL8J5zbhvv/f8CvwA+5Zzr7b3/VyQoPu2cG+BF76N6QPWbu1Aw+tA5NwpVLP1SSwsF+MjxPRW4ARXQ+ymKOupnm/QDBrmssmuizYASlJSo2ch7/5RzbgbwinNuJFbuAjjIMmN7AP9rI+xzcSGTbYNk9YneBj7rvX+3NbKHncqSnIbagK4F1jrntgNWO+dWo8ZFC0yIJ9pMKEFJiZqdnHNHoSY7+6C+wtNQA5duKProoQKnlygip0J9TwCXe++fjKCletSxbQtLZkslLjYjShZDomYn7/1K8yGsA/bzanwzAviz9/4fE5NpO+S9/6Nz7gFgvHPuF15VXcehng9PGASYSlxsZpQshkQtRk7d2O4Ehnvvf1v0fBKVJ1dFt7lEmwclwZCoRcmpIc6fvPfPFD2XRI2TQUr7odDh98zfkGgzpSQYErUKJfgoUaL2Q0kwJEqUKFGiEkp5DIkSJUqUqISSYEiUKFGiRCWUBEOiRIkSJSqhJBgSJUqUKFEJJcGQKFGiRIlKKAmGRIkSJUpUQkkwJEqUKFGiEvp/YVTFguDW9pUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = DF.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.03643216e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.00000000e+02, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [3.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         1.69760692e-01, 0.00000000e+00, 1.64284541e-02],\n",
       "        [3.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         6.67815597e-01, 5.92417062e-02, 1.07712193e-01],\n",
       "        [3.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "         6.67815597e-01, 5.92417062e-02, 1.07712193e-01]]),\n",
       " ['course',\n",
       "  'completed',\n",
       "  'upgraded',\n",
       "  'gender',\n",
       "  'country',\n",
       "  'age',\n",
       "  'education',\n",
       "  'job',\n",
       "  'major',\n",
       "  'logged_location',\n",
       "  'withdrew',\n",
       "  'registration',\n",
       "  'withdrawl',\n",
       "  'completion',\n",
       "  'subscribers',\n",
       "  'subscribe_rate',\n",
       "  'churn_rate',\n",
       "  'upgrade_rate',\n",
       "  'completion_rate'],\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " 19)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(DF['upgraded'])\n",
    "DF = DF.drop(['upgrade'], axis=1)\n",
    "features = np.array(DF)\n",
    "feature_names = DF.columns.tolist()\n",
    "\n",
    "features, feature_names, labels, len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important to normalize data\n",
    "features = (features - features.mean(axis=0, keepdims=True)) / features.std(axis=0, keepdims=True)\n",
    "# features.shape, labels.shape\n",
    "# DF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,\n",
       " (tensor([-1.0690, -0.2904, -0.0918, -0.2690, -0.2401, -0.2335, -0.2778, -0.2620,\n",
       "          -0.2315, -0.8783,  2.6506,  0.7043, -1.1006, -0.4640, -1.5216,  1.9984,\n",
       "           2.6093, -0.4849, -0.4029]),\n",
       "  tensor(0)),\n",
       " 19)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MOOC(Dataset):\n",
    "    def __init__(self):\n",
    "        global features, labels\n",
    "        assert features.shape[0] == labels.shape[0], \"The lengths of features and labels do not match\"\n",
    "\n",
    "        self.feature_names = feature_names\n",
    "        self.features = torch.tensor(features, dtype=torch.float)\n",
    "        self.labels = torch.tensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.labels[idx])\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "dataset = MOOC()\n",
    "len(dataset), dataset[0], len(dataset.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2 dataloaders: a training set loader and a validation set loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209,\n",
       " 24791,\n",
       " 1,\n",
       " 375,\n",
       " 1,\n",
       " 42,\n",
       " [tensor([[-1.0690, -0.2904, -0.0918,  ...,  2.6093, -0.4849, -0.4029],\n",
       "          [-1.0690,  3.4438, 10.8912,  ..., -0.3189, -0.0214, -0.1126],\n",
       "          [ 1.6036, -0.2904, 10.8912,  ..., -0.4131,  0.2230, -0.3296],\n",
       "          ...,\n",
       "          [ 1.6036, -0.2904, -0.0918,  ..., -0.4255,  0.8281, -0.2191],\n",
       "          [-1.0690,  3.4438, 10.8912,  ..., -0.4019, -0.4849, -0.4029],\n",
       "          [-1.0690, -0.2904, -0.0918,  ...,  2.6093, -0.4849, -0.4029]]),\n",
       "  tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "          1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "          1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "          0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "          0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "          1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "          1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "          1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "          0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "          1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "          1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "          0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "          1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "          0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "          1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0])])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 0.1\n",
    "shuffle = True\n",
    "\n",
    "# Taking the same proportion of examples from both classes\n",
    "true_idx = dataset[:][1].eq(1).nonzero().view(-1).long()\n",
    "false_idx = dataset[:][1].eq(0).nonzero().view(-1).long()\n",
    "\n",
    "t_new_idx = true_idx[torch.randperm(len(true_idx))] if shuffle else true_idx\n",
    "f_new_idx = false_idx[torch.randperm(len(false_idx))] if shuffle else false_idx\n",
    "\n",
    "# trying out matching the number of examples for each class\n",
    "f_new_idx = f_new_idx[:(len(t_new_idx) // 2) * 2]\n",
    "\n",
    "t_test_len = int(round(len(t_new_idx) * test_size))\n",
    "f_test_len = int(round(len(f_new_idx) * test_size))\n",
    "train_idx = torch.cat((t_new_idx[:len(t_new_idx) - t_test_len], f_new_idx[:len(f_new_idx) - f_test_len]))\n",
    "validation_idx = torch.cat((t_new_idx[len(t_new_idx) - t_test_len:], f_new_idx[len(f_new_idx) - f_test_len:]))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size=len(train_sampler), sampler=train_sampler)\n",
    "validationloader = DataLoader(dataset, batch_size=len(validation_sampler), sampler=validation_sampler)\n",
    "len(true_idx), len(false_idx), len(trainloader), len(train_sampler), len(validationloader), len(validation_sampler), next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim = 9, output_dim = 1):\n",
    "        super(ANN, self).__init__()\n",
    "    \n",
    "        # Input Layer (784) -> 784\n",
    "        self.fc1 = nn.Linear(input_dim, 200)\n",
    "        # 64 -> 64\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        # 64 -> 32\n",
    "        self.fc3 = nn.Linear(200, 100)\n",
    "        # 32 -> 32\n",
    "        self.fc4 = nn.Linear(100, 100)\n",
    "        # 32 -> output layer(10)\n",
    "        self.output_layer = nn.Linear(100,1)\n",
    "        # Dropout Layer (20%) to reduce overfitting\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    # Feed Forward Function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Add ReLU activation function to each layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        # Don't add any ReLU activation function to Last Output Layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        # Return the created model\n",
    "        return nn.Sigmoid()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc1): Linear(in_features=19, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (output_layer): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Neural Network Model\n",
    "model = ANN(input_dim = 19, output_dim = 1)\n",
    "# Print its architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# specify loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9,nesterov = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.0107\t Acc: 49.87%\n",
      "Epoch: 2 \tTraining Loss: 0.0106\t Acc: 49.87%\n",
      "Epoch: 3 \tTraining Loss: 0.0106\t Acc: 49.87%\n",
      "Epoch: 4 \tTraining Loss: 0.0105\t Acc: 49.87%\n",
      "Epoch: 5 \tTraining Loss: 0.0105\t Acc: 49.87%\n",
      "Epoch: 6 \tTraining Loss: 0.0104\t Acc: 49.87%\n",
      "Epoch: 7 \tTraining Loss: 0.0103\t Acc: 49.87%\n",
      "Epoch: 8 \tTraining Loss: 0.0102\t Acc: 49.87%\n",
      "Epoch: 9 \tTraining Loss: 0.0102\t Acc: 49.87%\n",
      "Epoch: 10 \tTraining Loss: 0.0101\t Acc: 50.13%\n",
      "Epoch: 11 \tTraining Loss: 0.0100\t Acc: 50.13%\n",
      "Epoch: 12 \tTraining Loss: 0.0099\t Acc: 50.13%\n",
      "Epoch: 13 \tTraining Loss: 0.0098\t Acc: 50.13%\n",
      "Epoch: 14 \tTraining Loss: 0.0097\t Acc: 50.13%\n",
      "Epoch: 15 \tTraining Loss: 0.0096\t Acc: 50.13%\n",
      "Epoch: 16 \tTraining Loss: 0.0094\t Acc: 50.13%\n",
      "Epoch: 17 \tTraining Loss: 0.0093\t Acc: 50.13%\n",
      "Epoch: 18 \tTraining Loss: 0.0091\t Acc: 50.13%\n",
      "Epoch: 19 \tTraining Loss: 0.0089\t Acc: 50.13%\n",
      "Epoch: 20 \tTraining Loss: 0.0087\t Acc: 50.13%\n",
      "Epoch: 21 \tTraining Loss: 0.0084\t Acc: 50.13%\n",
      "Epoch: 22 \tTraining Loss: 0.0082\t Acc: 50.13%\n",
      "Epoch: 23 \tTraining Loss: 0.0079\t Acc: 50.13%\n",
      "Epoch: 24 \tTraining Loss: 0.0077\t Acc: 50.13%\n",
      "Epoch: 25 \tTraining Loss: 0.0074\t Acc: 50.13%\n",
      "Epoch: 26 \tTraining Loss: 0.0072\t Acc: 50.13%\n",
      "Epoch: 27 \tTraining Loss: 0.0068\t Acc: 50.13%\n",
      "Epoch: 28 \tTraining Loss: 0.0066\t Acc: 50.13%\n",
      "Epoch: 29 \tTraining Loss: 0.0065\t Acc: 50.13%\n",
      "Epoch: 30 \tTraining Loss: 0.0063\t Acc: 50.13%\n",
      "Epoch: 31 \tTraining Loss: 0.0061\t Acc: 50.13%\n",
      "Epoch: 32 \tTraining Loss: 0.0060\t Acc: 50.13%\n",
      "Epoch: 33 \tTraining Loss: 0.0058\t Acc: 50.13%\n",
      "Epoch: 34 \tTraining Loss: 0.0056\t Acc: 50.13%\n",
      "Epoch: 35 \tTraining Loss: 0.0055\t Acc: 50.13%\n",
      "Epoch: 36 \tTraining Loss: 0.0053\t Acc: 49.87%\n",
      "Epoch: 37 \tTraining Loss: 0.0052\t Acc: 49.87%\n",
      "Epoch: 38 \tTraining Loss: 0.0050\t Acc: 50.13%\n",
      "Epoch: 39 \tTraining Loss: 0.0048\t Acc: 49.87%\n",
      "Epoch: 40 \tTraining Loss: 0.0046\t Acc: 49.87%\n",
      "Epoch: 41 \tTraining Loss: 0.0045\t Acc: 50.13%\n",
      "Epoch: 42 \tTraining Loss: 0.0043\t Acc: 50.13%\n",
      "Epoch: 43 \tTraining Loss: 0.0041\t Acc: 50.13%\n",
      "Epoch: 44 \tTraining Loss: 0.0040\t Acc: 49.87%\n",
      "Epoch: 45 \tTraining Loss: 0.0038\t Acc: 49.87%\n",
      "Epoch: 46 \tTraining Loss: 0.0037\t Acc: 49.87%\n",
      "Epoch: 47 \tTraining Loss: 0.0036\t Acc: 49.87%\n",
      "Epoch: 48 \tTraining Loss: 0.0035\t Acc: 50.13%\n",
      "Epoch: 49 \tTraining Loss: 0.0034\t Acc: 50.13%\n",
      "Epoch: 50 \tTraining Loss: 0.0033\t Acc: 49.87%\n",
      "Epoch: 51 \tTraining Loss: 0.0031\t Acc: 49.87%\n",
      "Epoch: 52 \tTraining Loss: 0.0031\t Acc: 50.13%\n",
      "Epoch: 53 \tTraining Loss: 0.0030\t Acc: 49.87%\n",
      "Epoch: 54 \tTraining Loss: 0.0029\t Acc: 49.87%\n",
      "Epoch: 55 \tTraining Loss: 0.0028\t Acc: 49.87%\n",
      "Epoch: 56 \tTraining Loss: 0.0027\t Acc: 50.13%\n",
      "Epoch: 57 \tTraining Loss: 0.0026\t Acc: 49.87%\n",
      "Epoch: 58 \tTraining Loss: 0.0026\t Acc: 50.13%\n",
      "Epoch: 59 \tTraining Loss: 0.0025\t Acc: 49.87%\n",
      "Epoch: 60 \tTraining Loss: 0.0024\t Acc: 50.13%\n",
      "Epoch: 61 \tTraining Loss: 0.0024\t Acc: 50.13%\n",
      "Epoch: 62 \tTraining Loss: 0.0023\t Acc: 49.87%\n",
      "Epoch: 63 \tTraining Loss: 0.0022\t Acc: 49.87%\n",
      "Epoch: 64 \tTraining Loss: 0.0021\t Acc: 50.13%\n",
      "Epoch: 65 \tTraining Loss: 0.0021\t Acc: 49.87%\n",
      "Epoch: 66 \tTraining Loss: 0.0020\t Acc: 50.13%\n",
      "Epoch: 67 \tTraining Loss: 0.0020\t Acc: 49.87%\n",
      "Epoch: 68 \tTraining Loss: 0.0019\t Acc: 50.13%\n",
      "Epoch: 69 \tTraining Loss: 0.0019\t Acc: 49.87%\n",
      "Epoch: 70 \tTraining Loss: 0.0018\t Acc: 50.13%\n",
      "Epoch: 71 \tTraining Loss: 0.0017\t Acc: 50.13%\n",
      "Epoch: 72 \tTraining Loss: 0.0017\t Acc: 50.13%\n",
      "Epoch: 73 \tTraining Loss: 0.0016\t Acc: 49.87%\n",
      "Epoch: 74 \tTraining Loss: 0.0016\t Acc: 50.13%\n",
      "Epoch: 75 \tTraining Loss: 0.0015\t Acc: 50.13%\n",
      "Epoch: 76 \tTraining Loss: 0.0015\t Acc: 50.13%\n",
      "Epoch: 77 \tTraining Loss: 0.0015\t Acc: 49.87%\n",
      "Epoch: 78 \tTraining Loss: 0.0014\t Acc: 50.13%\n",
      "Epoch: 79 \tTraining Loss: 0.0014\t Acc: 50.13%\n",
      "Epoch: 80 \tTraining Loss: 0.0013\t Acc: 50.13%\n",
      "Epoch: 81 \tTraining Loss: 0.0013\t Acc: 49.87%\n",
      "Epoch: 82 \tTraining Loss: 0.0012\t Acc: 50.13%\n",
      "Epoch: 83 \tTraining Loss: 0.0012\t Acc: 49.87%\n",
      "Epoch: 84 \tTraining Loss: 0.0012\t Acc: 49.87%\n",
      "Epoch: 85 \tTraining Loss: 0.0011\t Acc: 50.13%\n",
      "Epoch: 86 \tTraining Loss: 0.0011\t Acc: 49.87%\n",
      "Epoch: 87 \tTraining Loss: 0.0011\t Acc: 49.87%\n",
      "Epoch: 88 \tTraining Loss: 0.0010\t Acc: 49.87%\n",
      "Epoch: 89 \tTraining Loss: 0.0010\t Acc: 49.87%\n",
      "Epoch: 90 \tTraining Loss: 0.0010\t Acc: 50.13%\n",
      "Epoch: 91 \tTraining Loss: 0.0009\t Acc: 49.87%\n",
      "Epoch: 92 \tTraining Loss: 0.0009\t Acc: 50.13%\n",
      "Epoch: 93 \tTraining Loss: 0.0009\t Acc: 50.13%\n",
      "Epoch: 94 \tTraining Loss: 0.0008\t Acc: 49.87%\n",
      "Epoch: 95 \tTraining Loss: 0.0008\t Acc: 49.87%\n",
      "Epoch: 96 \tTraining Loss: 0.0008\t Acc: 49.87%\n",
      "Epoch: 97 \tTraining Loss: 0.0008\t Acc: 50.13%\n",
      "Epoch: 98 \tTraining Loss: 0.0007\t Acc: 49.87%\n",
      "Epoch: 99 \tTraining Loss: 0.0007\t Acc: 49.87%\n",
      "Epoch: 100 \tTraining Loss: 0.0007\t Acc: 50.13%\n",
      "Epoch: 101 \tTraining Loss: 0.0007\t Acc: 50.13%\n",
      "Epoch: 102 \tTraining Loss: 0.0007\t Acc: 49.87%\n",
      "Epoch: 103 \tTraining Loss: 0.0006\t Acc: 49.87%\n",
      "Epoch: 104 \tTraining Loss: 0.0006\t Acc: 49.87%\n",
      "Epoch: 105 \tTraining Loss: 0.0006\t Acc: 50.13%\n",
      "Epoch: 106 \tTraining Loss: 0.0006\t Acc: 50.13%\n",
      "Epoch: 107 \tTraining Loss: 0.0006\t Acc: 49.87%\n",
      "Epoch: 108 \tTraining Loss: 0.0005\t Acc: 50.13%\n",
      "Epoch: 109 \tTraining Loss: 0.0005\t Acc: 49.87%\n",
      "Epoch: 110 \tTraining Loss: 0.0005\t Acc: 49.87%\n",
      "Epoch: 111 \tTraining Loss: 0.0005\t Acc: 49.87%\n",
      "Epoch: 112 \tTraining Loss: 0.0005\t Acc: 49.87%\n",
      "Epoch: 113 \tTraining Loss: 0.0005\t Acc: 49.87%\n",
      "Epoch: 114 \tTraining Loss: 0.0005\t Acc: 49.87%\n",
      "Epoch: 115 \tTraining Loss: 0.0004\t Acc: 50.13%\n",
      "Epoch: 116 \tTraining Loss: 0.0004\t Acc: 50.13%\n",
      "Epoch: 117 \tTraining Loss: 0.0004\t Acc: 49.87%\n",
      "Epoch: 118 \tTraining Loss: 0.0004\t Acc: 49.87%\n",
      "Epoch: 119 \tTraining Loss: 0.0004\t Acc: 49.87%\n",
      "Epoch: 120 \tTraining Loss: 0.0004\t Acc: 49.87%\n",
      "Epoch: 121 \tTraining Loss: 0.0004\t Acc: 50.13%\n",
      "Epoch: 122 \tTraining Loss: 0.0004\t Acc: 50.13%\n",
      "Epoch: 123 \tTraining Loss: 0.0003\t Acc: 49.87%\n",
      "Epoch: 124 \tTraining Loss: 0.0003\t Acc: 50.13%\n",
      "Epoch: 125 \tTraining Loss: 0.0003\t Acc: 49.87%\n",
      "Epoch: 126 \tTraining Loss: 0.0003\t Acc: 50.13%\n",
      "Epoch: 127 \tTraining Loss: 0.0003\t Acc: 50.13%\n",
      "Epoch: 128 \tTraining Loss: 0.0003\t Acc: 50.13%\n",
      "Epoch: 129 \tTraining Loss: 0.0003\t Acc: 50.13%\n",
      "Epoch: 130 \tTraining Loss: 0.0003\t Acc: 49.87%\n",
      "Epoch: 131 \tTraining Loss: 0.0003\t Acc: 49.87%\n",
      "Epoch: 132 \tTraining Loss: 0.0003\t Acc: 50.13%\n",
      "Epoch: 133 \tTraining Loss: 0.0003\t Acc: 49.87%\n",
      "Epoch: 134 \tTraining Loss: 0.0003\t Acc: 50.13%\n",
      "Epoch: 135 \tTraining Loss: 0.0003\t Acc: 49.87%\n",
      "Epoch: 136 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 137 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 138 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 139 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 140 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 141 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 142 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 143 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 144 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 145 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 146 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 147 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 148 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 149 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 150 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 151 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 152 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 153 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 154 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 155 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 156 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 157 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 158 \tTraining Loss: 0.0002\t Acc: 49.87%\n",
      "Epoch: 159 \tTraining Loss: 0.0002\t Acc: 50.13%\n",
      "Epoch: 160 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 161 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 162 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 163 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 164 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 165 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 166 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 167 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 168 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 169 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 170 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 171 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 172 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 173 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 174 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 175 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 176 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 177 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 178 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 179 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 180 \tTraining Loss: 0.0001\t Acc: 49.87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 182 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 183 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 184 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 185 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 186 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 187 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 188 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 189 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 190 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 191 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 192 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 193 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 194 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 195 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 196 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 197 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 198 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 199 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 200 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 201 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 202 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 203 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 204 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 205 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 206 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 207 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 208 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 209 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 210 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 211 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 212 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 213 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 214 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 215 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 216 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 217 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 218 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 219 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 220 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 221 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 222 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 223 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 224 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 225 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 226 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 227 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 228 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 229 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 230 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 231 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 232 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 233 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 234 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 235 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 236 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 237 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 238 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 239 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 240 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 241 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 242 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 243 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 244 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 245 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 246 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 247 \tTraining Loss: 0.0001\t Acc: 50.13%\n",
      "Epoch: 248 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 249 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 250 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 251 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 252 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 253 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 254 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 255 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 256 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 257 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 258 \tTraining Loss: 0.0001\t Acc: 49.87%\n",
      "Epoch: 259 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 260 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 261 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 262 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 263 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 264 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 265 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 266 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 267 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 268 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 269 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 270 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 271 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 272 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 273 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 274 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 275 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 276 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 277 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 278 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 279 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 280 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 281 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 282 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 283 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 284 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 285 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 286 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 287 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 288 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 289 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 290 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 291 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 292 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 293 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 294 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 295 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 296 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 297 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 298 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 299 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 300 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 301 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 302 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 303 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 304 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 305 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 306 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 307 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 308 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 309 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 310 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 311 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 312 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 313 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 314 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 315 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 316 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 317 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 318 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 319 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 320 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 321 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 322 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 323 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 324 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 325 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 326 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 327 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 328 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 329 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 330 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 331 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 332 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 333 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 334 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 335 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 336 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 337 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 338 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 339 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 340 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 341 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 342 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 343 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 344 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 345 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 346 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 347 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 348 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 349 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 350 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 351 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 352 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 353 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 354 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 355 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 356 \tTraining Loss: 0.0000\t Acc: 49.87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 357 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 358 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 359 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 360 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 361 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 362 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 363 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 364 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 365 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 366 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 367 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 368 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 369 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 370 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 371 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 372 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 373 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 374 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 375 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 376 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 377 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 378 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 379 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 380 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 381 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 382 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 383 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 384 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 385 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 386 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 387 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 388 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 389 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 390 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 391 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 392 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 393 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 394 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 395 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 396 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 397 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 398 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 399 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 400 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 401 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 402 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 403 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 404 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 405 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 406 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 407 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 408 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 409 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 410 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 411 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 412 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 413 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 414 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 415 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 416 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 417 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 418 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 419 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 420 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 421 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 422 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 423 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 424 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 425 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 426 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 427 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 428 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 429 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 430 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 431 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 432 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 433 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 434 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 435 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 436 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 437 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 438 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 439 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 440 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 441 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 442 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 443 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 444 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 445 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 446 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 447 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 448 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 449 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 450 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 451 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 452 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 453 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 454 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 455 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 456 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 457 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 458 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 459 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 460 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 461 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 462 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 463 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 464 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 465 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 466 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 467 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 468 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 469 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 470 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 471 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 472 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 473 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 474 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 475 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 476 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 477 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 478 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 479 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 480 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 481 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 482 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 483 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 484 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 485 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 486 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 487 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 488 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 489 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 490 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 491 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 492 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 493 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 494 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 495 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 496 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 497 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 498 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 499 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 500 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 501 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 502 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 503 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 504 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 505 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 506 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 507 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 508 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 509 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 510 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 511 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 512 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 513 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 514 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 515 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 516 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 517 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 518 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 519 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 520 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 521 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 522 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 523 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 524 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 525 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 526 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 527 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 528 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 529 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 530 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 531 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 532 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 533 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 534 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 535 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 536 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 537 \tTraining Loss: 0.0000\t Acc: 50.13%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 538 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 539 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 540 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 541 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 542 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 543 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 544 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 545 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 546 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 547 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 548 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 549 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 550 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 551 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 552 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 553 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 554 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 555 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 556 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 557 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 558 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 559 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 560 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 561 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 562 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 563 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 564 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 565 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 566 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 567 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 568 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 569 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 570 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 571 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 572 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 573 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 574 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 575 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 576 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 577 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 578 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 579 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 580 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 581 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 582 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 583 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 584 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 585 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 586 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 587 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 588 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 589 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 590 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 591 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 592 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 593 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 594 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 595 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 596 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 597 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 598 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 599 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 600 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 601 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 602 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 603 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 604 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 605 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 606 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 607 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 608 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 609 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 610 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 611 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 612 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 613 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 614 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 615 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 616 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 617 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 618 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 619 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 620 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 621 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 622 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 623 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 624 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 625 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 626 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 627 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 628 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 629 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 630 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 631 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 632 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 633 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 634 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 635 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 636 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 637 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 638 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 639 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 640 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 641 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 642 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 643 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 644 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 645 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 646 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 647 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 648 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 649 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 650 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 651 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 652 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 653 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 654 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 655 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 656 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 657 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 658 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 659 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 660 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 661 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 662 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 663 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 664 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 665 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 666 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 667 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 668 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 669 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 670 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 671 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 672 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 673 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 674 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 675 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 676 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 677 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 678 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 679 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 680 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 681 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 682 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 683 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 684 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 685 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 686 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 687 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 688 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 689 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 690 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 691 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 692 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 693 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 694 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 695 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 696 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 697 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 698 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 699 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 700 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 701 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 702 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 703 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 704 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 705 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 706 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 707 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 708 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 709 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 710 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 711 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 712 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 713 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 714 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 715 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 716 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 717 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 718 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 719 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 720 \tTraining Loss: 0.0000\t Acc: 49.87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 721 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 722 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 723 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 724 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 725 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 726 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 727 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 728 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 729 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 730 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 731 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 732 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 733 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 734 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 735 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 736 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 737 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 738 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 739 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 740 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 741 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 742 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 743 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 744 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 745 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 746 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 747 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 748 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 749 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 750 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 751 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 752 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 753 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 754 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 755 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 756 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 757 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 758 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 759 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 760 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 761 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 762 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 763 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 764 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 765 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 766 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 767 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 768 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 769 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 770 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 771 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 772 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 773 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 774 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 775 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 776 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 777 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 778 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 779 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 780 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 781 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 782 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 783 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 784 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 785 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 786 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 787 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 788 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 789 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 790 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 791 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 792 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 793 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 794 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 795 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 796 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 797 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 798 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 799 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 800 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 801 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 802 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 803 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 804 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 805 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 806 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 807 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 808 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 809 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 810 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 811 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 812 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 813 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 814 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 815 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 816 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 817 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 818 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 819 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 820 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 821 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 822 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 823 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 824 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 825 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 826 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 827 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 828 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 829 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 830 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 831 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 832 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 833 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 834 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 835 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 836 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 837 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 838 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 839 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 840 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 841 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 842 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 843 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 844 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 845 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 846 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 847 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 848 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 849 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 850 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 851 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 852 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 853 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 854 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 855 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 856 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 857 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 858 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 859 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 860 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 861 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 862 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 863 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 864 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 865 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 866 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 867 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 868 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 869 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 870 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 871 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 872 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 873 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 874 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 875 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 876 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 877 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 878 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 879 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 880 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 881 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 882 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 883 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 884 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 885 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 886 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 887 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 888 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 889 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 890 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 891 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 892 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 893 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 894 \tTraining Loss: 0.0000\t Acc: 49.87%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 895 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 896 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 897 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 898 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 899 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 900 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 901 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 902 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 903 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 904 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 905 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 906 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 907 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 908 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 909 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 910 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 911 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 912 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 913 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 914 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 915 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 916 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 917 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 918 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 919 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 920 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 921 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 922 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 923 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 924 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 925 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 926 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 927 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 928 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 929 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 930 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 931 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 932 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 933 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 934 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 935 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 936 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 937 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 938 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 939 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 940 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 941 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 942 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 943 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 944 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 945 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 946 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 947 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 948 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 949 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 950 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 951 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 952 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 953 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 954 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 955 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 956 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 957 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 958 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 959 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 960 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 961 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 962 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 963 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 964 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 965 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 966 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 967 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 968 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 969 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 970 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 971 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 972 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 973 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 974 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 975 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 976 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 977 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 978 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 979 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 980 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 981 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 982 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 983 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 984 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 985 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 986 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 987 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 988 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 989 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 990 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 991 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 992 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 993 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 994 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 995 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 996 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 997 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 998 \tTraining Loss: 0.0000\t Acc: 49.87%\n",
      "Epoch: 999 \tTraining Loss: 0.0000\t Acc: 50.13%\n",
      "Epoch: 1000 \tTraining Loss: 0.0000\t Acc: 50.13%\n"
     ]
    }
   ],
   "source": [
    "# Define epochs (between 20-50)\n",
    "epochs = 1000\n",
    "\n",
    "# Some lists to keep track of loss and accuracy during each epoch\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "# Set the training mode ON -> Activate Dropout Layers\n",
    "model.train() # prepare model for training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    \n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data,target in trainloader:\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.FloatTensor)\n",
    "        #print(\"Target = \",target[0].item())\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        predicted = (torch.round(output.data[0]))\n",
    "        # Total number of labels\n",
    "        total += len(target)\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == target).sum()\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = loss_fn(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    # calculate average training loss over an epoch\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    \n",
    "    # Avg Accuracy\n",
    "    accuracy = 100 * correct / float(total)\n",
    "    # Put them in their list\n",
    "    train_acc_list.append(accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f}\\t Acc: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        accuracy\n",
    "        ))\n",
    "    # Move to next epoch\n",
    "    epoch_list.append(epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Visualising the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list,train_loss_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Number of Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list,train_acc_list)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kusalh\\AppData\\Local\\Continuum\\miniconda3\\envs\\course\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "val_loss = 0\n",
    "model.eval() # Required for Evaluation/Test\n",
    "with torch.no_grad():\n",
    "    for data, target in validationloader:\n",
    "\n",
    "        # Convert our images and labels to Variables to accumulate Gradients\n",
    "        data = Variable(data).float()\n",
    "        target = Variable(target).type(torch.FloatTensor)\n",
    "\n",
    "        # Predict Output\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(output, target)\n",
    "        val_loss += loss.item()*data.size(0)\n",
    "        # Get predictions from the maximum value\n",
    "        predicted = (torch.round(output.data[0]))\n",
    "\n",
    "        # Total number of labels\n",
    "        total += len(target)\n",
    "\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == target).sum()\n",
    "    \n",
    "    # calculate average training loss and accuracy over an epoch\n",
    "    val_loss = val_loss/len(validationloader.dataset)\n",
    "    accuracy = 100 * correct/ float(total)\n",
    "print(\"Accuracy = \",accuracy.item() * 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
